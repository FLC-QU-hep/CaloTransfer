{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import preprocessing_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaloChallenge Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing consists of :\n",
    "1. Same impact energie and # points:\n",
    "\n",
    "    1.1. Conversion in GeV  \\\n",
    "    1.2. impact energy range [ 1, 1000 ] GeV  \\\n",
    "\n",
    "2. Smearing\n",
    "\n",
    "3. Reordering by increasing # points \n",
    "\n",
    "4. Removing sampling fraction rescaling:\n",
    " multiply point energies by 0.033\n",
    "\n",
    "The goal is to make as much as possible \n",
    "In the cell below if `train` is activated we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "# n_datasets = [1, 2, 3] -> for the training\n",
    "# n_datasets = [4] -> for the testing\n",
    "\n",
    "train = True\n",
    "\n",
    "if train:\n",
    "    n_datasets = [1]\n",
    "    print(\"Loading the training data...\\n\")\n",
    "else:\n",
    "    n_datasets = [4]\n",
    "    print(\"Loading the testing data...\\n\")\n",
    "\n",
    "\n",
    "data = utils.loading_files(path = \"/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/original\", \n",
    "                           n_datasets = n_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 conversion in GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data: conversion in GeV\n",
    "\n",
    "data = utils.normalize_incident(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2-1.3 Impact energies in the range chosen impact energy range GeV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = utils.filter_events(data,  energy_range=(500, 800))\n",
    "for i in range(len(filtered_data)):\n",
    "    print()\n",
    "    print(filtered_data[i]['showers'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to select only some indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train is False:\n",
    "    # Generate random indices\n",
    "\n",
    "    # 1. Prendo il dizionario che sta dentro la lista\n",
    "    data = filtered_data[0]\n",
    "\n",
    "    # 2. Genero 10.000 indici unici basati sul numero di righe di 'showers'\n",
    "    indices = np.random.choice(data['showers'].shape[0], size=10000, replace=False)\n",
    "\n",
    "    # 3. Campiono entrambe le matrici usando gli stessi indici\n",
    "    data['showers']  = data['showers'][indices]\n",
    "    data['incident'] = data['incident'][indices]\n",
    "    print(data['showers'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data in point cloud\n",
    "point_cloud = utils.to_point_cloud(filtered_data)\n",
    "# del data, n_datasets\n",
    "utils.free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plt_scatter(point_cloud['showers'][-1])\n",
    "utils.plt_scatter_2(point_cloud['showers'][-1])\n",
    "# utils.plt_scatter_2(utils.to_cylindrical(point_cloud['showers'][1]), cylindrical=True, title=\"Cylindrical coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Smearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "        # Use a serif font that's likely available\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['DejaVu Serif', 'Liberation Serif', 'Computer Modern Roman', 'Bitstream Vera Serif'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 12,\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 600,  # Higher DPI for publication quality\n",
    "        'savefig.format': 'pdf',  # PDF format is often preferred for publications\n",
    "        'savefig.bbox': 'tight',\n",
    "        'savefig.pad_inches': 0.1,\n",
    "        'axes.linewidth': 0.8,  # Slightly thinner axes lines\n",
    "        'lines.linewidth': 1.5,  # Slightly thicker plot lines\n",
    "        'lines.markersize': 4,  # Slightly smaller markers\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(point_cloud['showers'])):\n",
    "    if 500 <= point_cloud['incident'][i] <= 501:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the showers:', point_cloud['showers'].shape)\n",
    "for i in range(3):\n",
    "    print('events in {}:'.format(i), point_cloud['showers'][:, i].min(), point_cloud['showers'][:, i].max())\n",
    "    \n",
    "utils.free_memory()\n",
    "\n",
    "if train: \n",
    "    print('=== incident energy:', point_cloud['incident'][209])\n",
    "\n",
    "    utils.plt_scatter_2(point_cloud['showers'][209], title='Not smeared')\n",
    "\n",
    "    cylindrical_smear = utils.data_smearing(point_cloud['showers'][:])\n",
    "    # smeared = utils.data_smearing(point_cloud['showers'][:209209], noise=209.35)\n",
    "    utils.plt_scatter_2(cylindrical_smear[209], title='w/ Cylindrical smearing')\n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        print('events in {} after the smearing:'.format(i), cylindrical_smear[:, i].min(), cylindrical_smear[:, i].max())\n",
    "else:\n",
    "    utils.plt_scatter_2(point_cloud['showers'][10], title='Evaluation data - Not smeared')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# Your existing matplotlib configuration\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['DejaVu Serif', 'Liberation Serif', 'Computer Modern Roman', 'Bitstream Vera Serif'],\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 600,\n",
    "    'savefig.format': 'pdf',\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.1,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'lines.linewidth': 1.5,\n",
    "    'lines.markersize': 4,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "def plt_smearing_comparison(original_shower, smeared_shower):\n",
    "    \"\"\"Plot before and after cylindrical smearing in XZ plane only\"\"\"\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Original data (left plot) - XZ plane\n",
    "    x_orig = original_shower[0, :]\n",
    "    z_orig = original_shower[2, :]\n",
    "    energy_orig = original_shower[3, :]  # Energy values for coloring\n",
    "    \n",
    "    # Smaller point sizes\n",
    "    sizes_orig = energy_orig * 0.2   # Much smaller scaling\n",
    "    \n",
    "    # Get the energy range for proper colorbar scaling\n",
    "    energy_min = min(energy_orig.min(), smeared_shower[3, :].min())\n",
    "    energy_max = max(energy_orig.max(), smeared_shower[3, :].max())\n",
    "    \n",
    "    print(f\"Energy range: {energy_min:.2f} to {energy_max:.2f}\")\n",
    "    \n",
    "    scatter1 = axs[0].scatter(x_orig, z_orig, s=sizes_orig, c=energy_orig, \n",
    "                             alpha=0.8, cmap='plasma', vmin=energy_min, vmax=energy_max)\n",
    "    axs[0].set_ylim(-18, 18)\n",
    "    axs[0].set_xlim(-18, 18)\n",
    "    axs[0].set_title('Before Smearing')\n",
    "    axs[0].set_aspect('equal')  # Balanced circles\n",
    "    \n",
    "    # Remove cartesian axes\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_yticks([])\n",
    "    axs[0].spines['top'].set_visible(False)\n",
    "    axs[0].spines['right'].set_visible(False)\n",
    "    axs[0].spines['bottom'].set_visible(False)\n",
    "    axs[0].spines['left'].set_visible(False)\n",
    "    \n",
    "    # Add concentric circles and radial lines\n",
    "    num_segments_x = 18\n",
    "    num_segments_z = 50\n",
    "    for radius in range(1, num_segments_x):\n",
    "        circle = plt.Circle((0, 0), radius, color='black', alpha=0.5, \n",
    "                          fill=False, linewidth=0.8)\n",
    "        axs[0].add_artist(circle)\n",
    "    \n",
    "    # Last circle with thicker line\n",
    "    circle = plt.Circle((0, 0), num_segments_x, color='black', alpha=0.5, \n",
    "                       fill=False, linewidth=2.0)\n",
    "    axs[0].add_artist(circle)\n",
    "    \n",
    "    # Add radial lines (central lines made thinner) - ALL BLACK\n",
    "    theta = np.linspace(0, 2*np.pi, num_segments_z + 1)\n",
    "    for i in range(num_segments_z):\n",
    "        axs[0].plot([0, 18*np.cos(theta[i])], [0, 18*np.sin(theta[i])], \n",
    "                   color='black', alpha=0.5, linewidth=0.4)\n",
    "    \n",
    "    # Smeared data (right plot) - XZ plane\n",
    "    x_smear = smeared_shower[0, :]\n",
    "    z_smear = smeared_shower[2, :]\n",
    "    energy_smear = smeared_shower[3, :]  # Energy values for coloring\n",
    "    \n",
    "    # Smaller point sizes\n",
    "    sizes_smear = energy_smear * 0.3 + 0.2  # Much smaller scaling\n",
    "    \n",
    "    scatter2 = axs[1].scatter(x_smear, z_smear, s=sizes_smear, c=energy_smear, \n",
    "                             alpha=1, cmap='plasma', vmin=energy_min, vmax=energy_max)\n",
    "    axs[1].set_ylim(-18, 18)\n",
    "    axs[1].set_xlim(-18, 18)\n",
    "    axs[1].set_title('After Smearing')\n",
    "    axs[1].set_aspect('equal')  # Balanced circles\n",
    "    \n",
    "    # Remove cartesian axes\n",
    "    axs[1].set_xticks([])\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].spines['top'].set_visible(False)\n",
    "    axs[1].spines['right'].set_visible(False)\n",
    "    axs[1].spines['bottom'].set_visible(False)\n",
    "    axs[1].spines['left'].set_visible(False)\n",
    "    \n",
    "    # Add concentric circles and radial lines for smeared\n",
    "    for radius in range(1, num_segments_x):\n",
    "        circle = plt.Circle((0, 0), radius, color='black', alpha=0.5, \n",
    "                          fill=False, linewidth=0.8)\n",
    "        axs[1].add_artist(circle)\n",
    "    \n",
    "    # Last circle with thicker line\n",
    "    circle = plt.Circle((0, 0), num_segments_x, color='black', alpha=0.5, \n",
    "                       fill=False, linewidth=2.0)\n",
    "    axs[1].add_artist(circle)\n",
    "    \n",
    "    # Add radial lines (central lines made thinner) - ALL BLACK\n",
    "    for i in range(num_segments_z):\n",
    "        axs[1].plot([0, 18*np.cos(theta[i])], [0, 18*np.sin(theta[i])], \n",
    "                   color='black', alpha=0.5, linewidth=0.4)\n",
    "    \n",
    "    # Turn off grid for both\n",
    "    for i in range(2):\n",
    "        axs[i].grid(False)\n",
    "    \n",
    "    # Add colorbar positioned to the right of the second plot only\n",
    "    cbar = plt.colorbar(scatter2, ax=axs[1], orientation='vertical', \n",
    "                       fraction=0.046, pad=0.1, shrink=1.0)\n",
    "    cbar.set_label('Energy [MeV]', rotation=270, labelpad=15)\n",
    "    \n",
    "    # Explicitly set colorbar limits to match actual data range\n",
    "    cbar.mappable.set_clim(vmin=energy_min, vmax=energy_max)\n",
    "    \n",
    "    # Add arrow between plots properly centered\n",
    "    arrow = FancyArrowPatch((0.44, 0.5), (0.56, 0.5),\n",
    "                           connectionstyle=\"arc3,rad=0\", arrowstyle='->',\n",
    "                           mutation_scale=25, color='black',\n",
    "                           transform=fig.transFigure)\n",
    "    fig.patches.append(arrow)\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    plt.savefig('./results/for_paper/smearing_comparison.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.show()\n",
    "\n",
    "# Your existing code with the new function\n",
    "print('Shape of the showers:', point_cloud['showers'].shape)\n",
    "for i in range(3):\n",
    "    print('events in {}:'.format(i), point_cloud['showers'][:, i].min(), point_cloud['showers'][:, i].max())\n",
    "    \n",
    "utils.free_memory()\n",
    "\n",
    "if train: \n",
    "    print('=== incident energy:', point_cloud['incident'][209])\n",
    "    \n",
    "    cylindrical_smear = utils.data_smearing(point_cloud['showers'][:])\n",
    "    \n",
    "    # Create the comparison plot\n",
    "    plt_smearing_comparison(point_cloud['showers'][209], cylindrical_smear[209])\n",
    "    \n",
    "    for i in range(3):\n",
    "        print('events in {} after the smearing:'.format(i), cylindrical_smear[:, i].min(), cylindrical_smear[:, i].max())\n",
    "else:\n",
    "    utils.plt_scatter_2(point_cloud['showers'][10], title='Evaluation data - Not smeared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reordering by npoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the training dataset is used, we consider the cylindrical smearing \n",
    "sorted_data, max_npoints = utils.sort_and_process(cylindrical_smear if train else point_cloud['showers'],\n",
    "                                                    point_cloud['incident'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract ordered data\n",
    "sorted_dataz = [np.count_nonzero(point_cloud['showers'][i, -1, :]) for i in range(len(point_cloud['showers']))]\n",
    "indices = list(range(len(sorted_dataz)))\n",
    "\n",
    "# Create the figure and axes for two plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot 1\n",
    "ax[0].scatter(x=indices, y=sorted_dataz, marker='o', color='r', s=1)\n",
    "ax[0].set_xlabel('Indices')\n",
    "ax[0].set_ylabel('# points')\n",
    "ax[0].set_title('Ordered showers by # points')\n",
    "ax[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend with maximum and average values for the first plot\n",
    "min_value_1 = np.min(sorted_dataz)\n",
    "max_value_1 = np.max(sorted_dataz)\n",
    "avg_value_1 = np.mean(sorted_dataz)\n",
    "ax[0].legend([f'Min: {min_value_1} \\nMax: {max_value_1} \\nAvg: {avg_value_1:.2f}']).set_title('# points')\n",
    "\n",
    "# Example data for the second plot\n",
    "# We need to define another set of data. In this example, I will use existing data.\n",
    "sorted_en = [point_cloud['incident'][i] for i in range(len(point_cloud['incident']))]\n",
    "\n",
    "# Plot 2\n",
    "ax[1].scatter(indices, sorted_en, marker='o', color='r', s=1)\n",
    "ax[1].set_xlabel('Indices')\n",
    "ax[1].set_ylabel('Incident Energies [GeV]')\n",
    "ax[1].set_title('Ordered incident energies by # points')\n",
    "ax[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend with maximum and average values for the second plot\n",
    "min_value_2 = np.min(sorted_en)\n",
    "max_value_2 = np.max(sorted_en)\n",
    "avg_value_2 = np.mean(sorted_en)\n",
    "ax[1].legend([f'Min: {min_value_2:.2f} \\nMax: {max_value_2:.2f} \\nAvg: {avg_value_2:.2f}']).set_title('Incidents E')\n",
    "\n",
    "# Show both plots side by side\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract ordered data\n",
    "sorted_data_2 = [np.count_nonzero(sorted_data['showers'][i, -1, :]) for i in range(len(sorted_data['showers']))]\n",
    "indices = list(range(len(sorted_data_2)))\n",
    "\n",
    "# Create the figure and axes for two plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot 1\n",
    "ax[0].scatter(x=indices, y=sorted_data_2, marker='o', color='r', s=1)\n",
    "ax[0].set_xlabel('Indices')\n",
    "ax[0].set_ylabel('# points')\n",
    "ax[0].set_title('Ordered showers by # points')\n",
    "ax[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend with maximum and average values for the first plot\n",
    "min_value_1 = np.min(sorted_data_2)\n",
    "max_value_1 = np.max(sorted_data_2)\n",
    "avg_value_1 = np.mean(sorted_data_2)\n",
    "ax[0].legend([f'Min: {min_value_1} \\nMax: {max_value_1} \\nAvg: {avg_value_1:.2f}']).set_title('# points')\n",
    "\n",
    "# Example data for the second plot\n",
    "# We need to define another set of data. In this example, I will use existing data.\n",
    "sorted_en = [sorted_data['incident'][i] for i in range(len(sorted_data['incident']))]\n",
    "\n",
    "# Plot 2\n",
    "ax[1].scatter(indices, sorted_en, marker='o', color='r', s=1)\n",
    "ax[1].set_xlabel('Indices')\n",
    "ax[1].set_ylabel('Incident Energies [GeV]')\n",
    "ax[1].set_title('Ordered incident energies by # points')\n",
    "ax[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend with maximum and average values for the second plot\n",
    "min_value_2 = np.min(sorted_en)\n",
    "max_value_2 = np.max(sorted_en)\n",
    "avg_value_2 = np.mean(sorted_en)\n",
    "ax[1].legend([f'Min: {min_value_2:.2f} \\nMax: {max_value_2:.2f} \\nAvg: {avg_value_2:.2f}']).set_title('Incidents E')\n",
    "\n",
    "# Show both plots side by side\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rescaling energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sorted_simple_smearing, sorted_incident_energy, sorted_indices, sorted_npoints, npoints\n",
    "utils.free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = utils.rescaling_e(sorted_data)\n",
    "print(f\"Shape of preprocessed data: {preprocessed_data['showers'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print('events in {}:'.format(i), preprocessed_data['showers'][:, i].min(), preprocessed_data['showers'][:, i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plt_scatter_2(preprocessed_data['showers'][-1], title='Preprocessed data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers = preprocessed_data['showers'].copy()\n",
    "\n",
    "hist_val = np.zeros((showers.shape[0], 45, 50, 18))\n",
    "\n",
    "Xmin, Xmax = -18, 18\n",
    "Ymin, Ymax = 0, 45\n",
    "Zmin, Zmax = -18, 18\n",
    "\n",
    "showers[:, 0, :] = (showers[:, 0, :] - Xmin) / (Xmax - Xmin) * 2 - 1\n",
    "showers[:, 1, :] = (showers[:, 1, :] - Ymin) / (Ymax - Ymin) * 2 - 1\n",
    "showers[:, 2, :] = (showers[:, 2, :] - Zmin) / (Zmax - Zmin) * 2 - 1\n",
    "\n",
    "\n",
    "val_events_reshaped = np.zeros((showers.shape[0], 45 * 50 * 18))\n",
    "for i in tqdm(range(showers.shape[0]), desc='reshaping into cylindrical'):\n",
    "    hist = utils.cylindrical_histogram(showers[i])\n",
    "    hist_reshaped = hist.reshape(45 * 50 * 18)\n",
    "    val_events_reshaped[i] = hist_reshaped\n",
    "\n",
    "for i in range(3):\n",
    "    print('events in {}:'.format(i), showers[:, i].min(), showers[:, i].max())\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_visible_e(dataset, log_scale=True, title=''):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the incident energies in the dataset.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    # TODO: add \n",
    "    # visible_energy = events[:, 3, :][events[:, 3, :] > 0]\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(dataset, bins=np.logspace(np.log(1e-7), np.log(dataset.max()), 200, base=np.e), alpha=0.75, color='blue', edgecolor='black')\n",
    "    \n",
    "    # Plot the mean line\n",
    "    mean_energy = dataset.mean()\n",
    "    plt.axvline(mean_energy, color='r', linestyle='dashed', linewidth=1, label=f'Mean energy: {mean_energy:.2f} GeV')\n",
    "    \n",
    "    # Optionally set a logarithmic scale for the y-axis\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Visible  Energy (MeV)')\n",
    "    plt.ylabel('Number of Events')\n",
    "    plt.title('Visible Energy' + title, fontsize=26)\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    plt.xlim(1e-4, dataset.max())\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the reshaped data: {val_events_reshaped.shape}\")\n",
    "visible_points = val_events_reshaped[:, 0:len(val_events_reshaped[0])][val_events_reshaped[:, 0:len(val_events_reshaped[0])] > 0]\n",
    "plt_visible_e(visible_points, title='Visible energy in the events')\n",
    "plt_visible_e(visible_points/0.033, title='Visible energy reshaped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_dict = {\n",
    "        'incident_energies': preprocessed_data['incident'],\n",
    "        # 'showers': val_events_reshaped,\n",
    "        'showers': showers,\n",
    "    }\n",
    "print(f\"Shape of preprocessed data: {val_data_dict['showers'].shape}\")\n",
    "# utils.plt_scatter_2(val_data_dict['showers'][1], title='Evaluation data - Not smeared')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_samples = len(showers)\n",
    "num_samples = 10000 # choose the number of samples in the training dataset here\n",
    "\n",
    "def get_percentage(total, part):\n",
    "        return part / total\n",
    "\n",
    "data_percentage = get_percentage(tot_samples, num_samples)\n",
    "if num_samples != tot_samples : \n",
    "    \n",
    "    print(f'the data percentage is: \\n {(data_percentage*100):.4f} % of total showers {tot_samples}')\n",
    "else:\n",
    "    print('the total training dataset is considered')\n",
    "\n",
    "if data_percentage < 1. and data_percentage > 0.:\n",
    "    size = int ( tot_samples * data_percentage )\n",
    "    idx = np.random.choice(tot_samples, size=size, replace=False)\n",
    "    idx = np.sort(idx).astype(int)\n",
    "    print(f'{(data_percentage*100):.2f} % of the dataset is considered!')\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_dict = {\n",
    "        'incident_energies': preprocessed_data['incident'][idx],\n",
    "        # 'showers': val_events_reshaped[idx],\n",
    "        'showers': showers[idx],\n",
    "\n",
    "    }\n",
    "print(f\"Shape of preprocessed data: {val_data_dict['showers'].shape}\")\n",
    "# utils.plt_scatter_2(val_data_dict['showers'][1], title='Evaluation data - Not smeared')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_data_dict['incident_energies'], bins=100, log=True, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in an external file\n",
    "\n",
    "val_file='10k_val_dset4_prep_1-1000GeV.hdf5'\n",
    "folder = '1-1000GeV/evaluation/'\n",
    "path = \"/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/\"\n",
    " \n",
    "path + folder + val_file\n",
    "utils.save_hdf5(val_data_dict, file_path= path + folder + val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_point_cloud = utils.to_point_cloud([val_data_dict])\n",
    "# utils.plt_scatter_2(eval_point_cloud['showers'][1], title='Evaluation data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plt_scatter_2(eval_point_cloud['showers'][-1], title='Evaluation data - Not smeared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plt_scatter_2(eval_point_cloud['showers'][-1], title='Evaluation data - Not smeared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/1-1000GeV/100k_train_dset1-2-3_prep_1-1000GeV.hdf5'\n",
    "with h5py.File(data, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Dataset disponibili nel file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers = f['events'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_energy = f['energy'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers.shape}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "data = '/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/1-1000GeV/evaluation/10k_val_dset4_prep_1-1000GeV.hdf5'\n",
    "with h5py.File(data, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Dataset disponibili nel file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers = f['showers'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_energy = f['incident_energies'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers.shape}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n_points = (showers[:][:, -1] > 0).sum(axis=1)\n",
    "# print(f\"Shape of the number of points: {n_points.shape}\")\n",
    "print(f\"min and maximum number of points: {n_points.min()} and {n_points.max()}\")\n",
    "print(f'min max impact energy: {impact_energy.min()} and {impact_energy.max()}')\n",
    "plt.hist(n_points, bins=100, log=True, color='blue', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing in the baseline value for Wasserstein and KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specifica il percorso del file\n",
    "file_path = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/val_dset4-prep_0-500GeV.hdf5'\n",
    "\n",
    "# Apri il file utilizzando h5py\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Dataset disponibili nel file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers_dset4 = f['showers'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_dset4 = f['incident_energies'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers_dset4.shape}\")        \n",
    "showers_dset4 /=0.033\n",
    "# showers_pc= utils.to_point_cloud([showers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specifica il percorso del file\n",
    "file_path = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/val_dset3-prep_0-500GeV.hdf5'\n",
    "\n",
    "# Apri il file utilizzando h5py\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Dataset disponibili nel file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers_dset3 = f['showers'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_dset3 = f['incident_energies'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers_dset3.shape}\") \n",
    "showers_dset3 /=0.033\n",
    "       \n",
    "# showers_pc= utils.to_point_cloud([showers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(len(showers_dset3) , size=len(showers_dset4), replace=False)\n",
    "\n",
    "showers_3_4_numpy = np.array([ showers_dset4, showers_dset3[indices]])\n",
    "showers_3_4_numpy[0].shape, showers_3_4_numpy[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.plot_evaluate as plot\n",
    "kl_divergences, wasserstein_dist = {}, {}\n",
    "kl_divergences, wasserstein_dist = plot.plot_visible_energy( showers_3_4_numpy, kl_divergences=kl_divergences, wasserstein=wasserstein_dist,log_scale = True)\n",
    "kl_divergences, wasserstein_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences, wasserstein_dist = plot.plot_nhits(showers_3_4_numpy, kl_divergences = kl_divergences, wasserstein=wasserstein_dist )\n",
    "kl_divergences, wasserstein_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences, wasserstein_dist = plot.plot_radial_energy(showers_3_4_numpy, kl_divergences = kl_divergences, wasserstein=wasserstein_dist )\n",
    "kl_divergences, wasserstein_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wasserstein_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wass = plot.plot_dataframe(wasserstein_dist,'Wasserstein distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plt_scatter(shower, title='Scatter Plots of Shower Coordinates'):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle( title , fontsize=26)\n",
    "\n",
    "    color = 'darkred'\n",
    "\n",
    "    axes[0].scatter(shower[0, :], shower[1, :], s=10, alpha=0.6, edgecolor='w', linewidth=0.5, color=color)\n",
    "    axes[0].set_xlabel('x', fontsize=22)\n",
    "    axes[0].set_ylabel('y', fontsize=22)\n",
    "    axes[0].set_title('x vs y', fontsize=24)\n",
    "\n",
    "    axes[1].scatter(shower[0, :], shower[2, :], s=10, alpha=0.6, edgecolor='w', linewidth=0.5, color=color)\n",
    "    axes[1].set_xlabel('x', fontsize=22)\n",
    "    axes[1].set_ylabel('z', fontsize=22)\n",
    "    axes[1].set_title('x vs z', fontsize=24)\n",
    "\n",
    "    axes[2].scatter(shower[1, :], shower[2, :], s=10, alpha=0.6, edgecolor='w', linewidth=0.5, color=color)\n",
    "    axes[2].set_xlabel('y', fontsize=22)\n",
    "    axes[2].set_ylabel('z', fontsize=22)\n",
    "    axes[2].set_title('y vs z', fontsize=24)\n",
    "\n",
    "    [axes[i].grid(False) for i in range(3)]\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Remove plt.show() so it doesn't automatically display the plot\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_pc = eval_point_cloud['showers']\n",
    "impact_energy = eval_point_cloud['incident']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_images = 25\n",
    "\n",
    "# Assume showers and impact_energy are defined\n",
    "indices = np.linspace(0, showers_pc.shape[0] - 1, n_images, dtype=int)\n",
    "frames = []\n",
    "\n",
    "for idx in indices:\n",
    "    n_points = np.count_nonzero(showers_pc[idx, -1, :])\n",
    "    title = f\"Shower {idx} - Energy {impact_energy[idx]} GeV - Number of points {n_points}\"\n",
    "    print(title)\n",
    "    fig = plt_scatter(showers_pc[idx], title=f\"Shower {idx} - Energy {impact_energy[idx]} GeV - Number of points {n_points}\")\n",
    "    \n",
    "    fig.canvas.draw()  # Ensure the drawing is complete\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    frames.append(image)\n",
    "    plt.close(fig)  # Close the figure to free up resources\n",
    "\n",
    "output_file = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/gif/val_dset4-prep_0-500GeV.gif'\n",
    "imageio.mimsave(output_file, frames, fps=0.5)\n",
    "\n",
    "print(f\"GIF saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalisation and saving training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers = preprocessed_data['showers'].copy()\n",
    "\n",
    "Xmin, Xmax = -18, 18\n",
    "Ymin, Ymax = 0, 45\n",
    "Zmin, Zmax = -18, 18\n",
    "\n",
    "showers[:, 0, :] = (showers[:, 0, :] - Xmin) *2 / (Xmax - Xmin)  - 1\n",
    "showers[:, 1, :] = (showers[:, 1, :] - Ymin) *2 / (Ymax - Ymin)  - 1\n",
    "showers[:, 2, :] = (showers[:, 2, :] - Zmin) *2 / (Zmax - Zmin)  - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {\n",
    "        'energy': preprocessed_data['incident'][1000:],\n",
    "        'events': showers[1000:],\n",
    "    }\n",
    "print(f\"Shape of preprocessed data: {train_data_dict['events'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='100k_train_dset1-2-3_prep_1-500GeV.hdf5'\n",
    "folder = '1-1000GeV/'\n",
    "path = \"/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/\"\n",
    "\n",
    "utils.save_hdf5(train_data_dict, file_path= path + folder + train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, showers, energy = utils.read_hdf5_file2(path + folder + train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/train_dset1-2_prep_10-90GeV.hdf5'\n",
    "_, energy, events = utils.read_hdf5_file2(path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making smaller dataset, choosing sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='100k_train_dset1-2-3_prep_1-500GeV.hdf5'\n",
    "folder = '1-1000GeV/'\n",
    "path = \"/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specifica il percorso del file\n",
    "# file_path = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/100_train_dset1-2_prep_0-500GeV.hdf5'\n",
    "file_path = path + folder + train_file\n",
    "# Apri il file utilizzando h5py\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Dataset disponibili nel file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers = f['events'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_energy = f['energy'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers.shape}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_samples = len(showers)\n",
    "num_samples = 1000 # choose the number of samples in the training dataset here\n",
    "\n",
    "def get_percentage(total, part):\n",
    "        return part / total\n",
    "\n",
    "data_percentage = get_percentage(tot_samples, num_samples)\n",
    "if num_samples != tot_samples : \n",
    "    \n",
    "    print(f'the data percentage is: \\n {(data_percentage*100):.4f} % of total {tot_samples}')\n",
    "else:\n",
    "    print('the total training dataset is considered')\n",
    "\n",
    "if data_percentage < 1. and data_percentage > 0.:\n",
    "    size = int ( tot_samples * data_percentage )\n",
    "    idx = np.random.choice(tot_samples, size=size, replace=False)\n",
    "    idx = np.sort(idx).astype(int)\n",
    "    print(f'{(data_percentage*100):.2f} % of the dataset is considered!')\n",
    "    print(idx)\n",
    "    \n",
    "else:\n",
    "    data_percentage = 1\n",
    "    idx = np.arange(2874, len(showers)).astype(int)\n",
    "    print('total daset is considered!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {\n",
    "        'energy': impact_energy[idx],\n",
    "        'events': showers[idx],\n",
    "    }\n",
    "print(f\"Shape of preprocessed data: {train_data_dict['events'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(impact_energy, bins=100, log=True, histtype='step',label= 'total samples' )\n",
    "plt.hist(train_data_dict['energy'], bins=100, log=True,  label=f'{(data_percentage*100):.1f} % samples' )\n",
    "plt.legend()\n",
    "plt.title('Impact Energies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_point= np.count_nonzero(train_data_dict['events'][:, -1, :], axis=-1)\n",
    "print('min and max number or points:', np.min(n_point), np.max(n_point))\n",
    "print('min and max energy:', f\"{np.min(train_data_dict['energy'][:]):.10g}\", f\"{np.max(train_data_dict['energy'][:]):.10g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the files\n",
    "train_file='train_dset1-2-3_prep_1-1000GeV.hdf5'\n",
    "path = \"/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/\"\n",
    "\n",
    "file_path = path + folder +  '1k_' +   train_file#'{:.0f}'.format(data_percentage*100) + 'k_' +   train_file\n",
    "file_path\n",
    "utils.save_hdf5(train_data_dict, file_path= file_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoint = [np.count_nonzero(train_data_dict['events'][i, -1, :]) for i in range(train_data_dict['events'].shape[0])]\n",
    "print(np.min(npoint), np.max(npoint))\n",
    "\n",
    "# visible_shower = train_data_dict['events'][(train_data_dict['events'] != 0) ]\n",
    "\n",
    "visible_energy = train_data_dict['events'][:, 3, :][train_data_dict['events'][:, 3, :] > 0]\n",
    "\n",
    "plt.hist(visible_energy , bins=np.logspace(np.log(1e-7), np.log(visible_energy.max()), 200, base=np.e))\n",
    "print(visible_energy.max())\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# plt.ylim(ymax=1e8)\n",
    "plt.xlim(xmin=1e-4)\n",
    "\n",
    "plt.xlabel('Visible Energy [MeV]')\n",
    "plt.ylabel('\\# cell')\n",
    "\n",
    "plt.legend(fontsize=20, frameon=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specifica il percorso del file\n",
    "file_path = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/train_dset1-2_prep_0-500GeV.hdf5'\n",
    "\n",
    "# Apri il file utilizzando h5py\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Dataset disponibili nel file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers = f['events'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_energy = f['energy'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers.shape}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plt_scatter(shower, title='Scatter Plots of Shower Coordinates'):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle( title , fontsize=26)\n",
    "\n",
    "    color = 'darkred'\n",
    "\n",
    "    axes[0].scatter(shower[0, :], shower[1, :], s=10, alpha=0.6, edgecolor='w', linewidth=0.5, color=color)\n",
    "    axes[0].set_xlabel('x', fontsize=22)\n",
    "    axes[0].set_ylabel('y', fontsize=22)\n",
    "    axes[0].set_title('x vs y', fontsize=24)\n",
    "\n",
    "    axes[1].scatter(shower[0, :], shower[2, :], s=10, alpha=0.6, edgecolor='w', linewidth=0.5, color=color)\n",
    "    axes[1].set_xlabel('x', fontsize=22)\n",
    "    axes[1].set_ylabel('z', fontsize=22)\n",
    "    axes[1].set_title('x vs z', fontsize=24)\n",
    "\n",
    "    axes[2].scatter(shower[1, :], shower[2, :], s=10, alpha=0.6, edgecolor='w', linewidth=0.5, color=color)\n",
    "    axes[2].set_xlabel('y', fontsize=22)\n",
    "    axes[2].set_ylabel('z', fontsize=22)\n",
    "    axes[2].set_title('y vs z', fontsize=24)\n",
    "\n",
    "    [axes[i].grid(False) for i in range(3)]\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Remove plt.show() so it doesn't automatically display the plot\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_images = 50\n",
    "\n",
    "# Assume showers and impact_energy are defined\n",
    "indices = np.linspace(0, showers.shape[0] - 1, n_images, dtype=int)\n",
    "frames = []\n",
    "\n",
    "for idx in indices:\n",
    "    n_points = np.count_nonzero(showers[idx, -1, :])\n",
    "    title = f\"Shower {idx} - Energy {impact_energy[idx]} GeV - Number of points {n_points}\"\n",
    "    print(title)\n",
    "    fig = plt_scatter(showers[idx], title=f\"Shower {idx} - Energy {impact_energy[idx]} GeV - Number of points {n_points}\")\n",
    "    \n",
    "    fig.canvas.draw()  # Ensure the drawing is complete\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    frames.append(image)\n",
    "    plt.close(fig)  # Close the figure to free up resources\n",
    "\n",
    "output_file = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/showers_animation.gif'\n",
    "imageio.mimsave(output_file, frames, fps=1)\n",
    "\n",
    "print(f\"GIF saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = utils.split_and_save_hdf5(preprocessed_data, validation_size=10000, \n",
    "                                       save = False,\n",
    "                                    train_file='train_prep_10-90GeV.hdf5',\n",
    "                                    validation_file='val_prep_10-90GeV.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing a fraction of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specifica il percorso del file\n",
    "# file_path = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/train_dset1-2_prep_0-500GeV.hdf5'\n",
    "file_path = '/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/10-90GeV/47k_dset1-2-3_prep_10-90GeV.hdf5'\n",
    "# Apri il file utilizzando h5py\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Lista dei dataset disponibili\n",
    "    print(\"Available keys in the file:\")\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Supponiamo che ci sia un dataset chiamato 'data'\n",
    "    # Puoi leggere i tuoi dati nel modo seguente\n",
    "    showers = f['events'][:]  # Questo scaricherà tutto il dataset in memoria\n",
    "    impact_energy = f['energy'][:]\n",
    "    print(f\"Shape del dataset 'showers': {showers.shape}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_samples = len(showers)\n",
    "num_samples = 5_000 # choose the number of samples in the training dataset here\n",
    "\n",
    "def get_percentage(total, part):\n",
    "        return part / total\n",
    "\n",
    "data_percentage = get_percentage(tot_samples, num_samples)\n",
    "if num_samples != tot_samples : \n",
    "    \n",
    "    print(f'the data percentage is: \\n {(data_percentage*100):.4f} % of total showers {tot_samples}')\n",
    "else:\n",
    "    print('the total training dataset is considered')\n",
    "\n",
    "if data_percentage < 1. and data_percentage > 0.:\n",
    "    size = int ( tot_samples * data_percentage )\n",
    "    idx = np.random.choice(tot_samples, size=size, replace=False)\n",
    "    idx = np.sort(idx).astype(int)\n",
    "    print(f'{(data_percentage*100):.2f} % of the dataset is considered!')\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {\n",
    "        'energy': impact_energy[idx],\n",
    "        'events': showers[idx],\n",
    "    }\n",
    "print(f\"Shape of preprocessed data: {train_data_dict['events'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_dict['energy'], bins=100, log=True, histtype='step',label= 'total samples' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the files\n",
    "train_file='dset1-2-3_prep_10-90GeV.hdf5'\n",
    "path = \"/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/10-90GeV/\"\n",
    "\n",
    "file_path = path + '5k' + '_' + train_file\n",
    "utils.save_hdf5(train_data_dict, file_path= file_path )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample choice for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming utils.read_hdf5_file2 is properly defined and provides correct data.\n",
    "path_validation = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/val_dset4-prep_0-500GeV.hdf5'\n",
    "_, _, incidents = utils.read_hdf5_file2(path_validation)\n",
    "incidents_numpy = np.array(incidents).ravel()\n",
    "\n",
    "sampling_sizes = [10000, 1000, 100]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(sampling_sizes), figsize=(18, 6), sharex=True, sharey=True)\n",
    "fig.suptitle('Analysis of Incident Energies with Different Sampling Sizes', fontsize=16)\n",
    "\n",
    "# Iterate over each sampling size and subplot\n",
    "for ax, size in zip(axes, sampling_sizes):\n",
    "    x = incidents_numpy.copy()\n",
    "    u = np.random.choice(x, size=size, replace=False)\n",
    "    \n",
    "    ax.hist(x, bins=100, log=True, histtype='step', label='Original')\n",
    "    ax.hist(u, bins=100, log=True, histtype='step', label=f'{size} log rand choice')\n",
    "\n",
    "    # Obtain histogram data to define bins\n",
    "    h, b = np.histogram(x, bins=100)\n",
    "\n",
    "    # Calculate bin indices and probabilities\n",
    "    bin_indices = np.digitize(x, bins=b[:-1], right=True) - 1\n",
    "    raw_probs = 1.0 / np.take(h, bin_indices, mode='clip')\n",
    "    probs = raw_probs / raw_probs.sum()\n",
    "\n",
    "    z = np.random.choice(x, size=size, replace=True, p=probs)  # Allow replacement since probabilities are used\n",
    "    ax.hist(z, bins=100, log=True, histtype='step', label=f'{size} unif rand choice')\n",
    "\n",
    "    ax.set_ylabel('Frequency (log scale)')\n",
    "    ax.set_xlabel('Incident energies [GeV]')\n",
    "    ax.legend(fontsize= 20)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # rect adjusts the space for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming utils.read_hdf5_file2 is properly defined and provides correct data.\n",
    "path_training = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/100_train_dset1-2_prep_0-500GeV.hdf5'\n",
    "_, incidents, _ = utils.read_hdf5_file2(path_training)\n",
    "incidents_numpy = np.array(incidents).ravel()\n",
    "\n",
    "sampling_sizes = [10000, 1000 ]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(sampling_sizes), figsize=(18, 6), sharex=True, sharey=True)\n",
    "fig.suptitle('Analysis of Incident Energies with Different Sampling Sizes', fontsize=26)\n",
    "\n",
    "# Iterate over each sampling size and subplot\n",
    "for ax, size in zip(axes, sampling_sizes):\n",
    "    x = incidents_numpy.copy()\n",
    "    u = np.random.choice(x, size=size, replace=False)\n",
    "    \n",
    "    ax.hist(x, bins=100, log=True, histtype='step', label='Original')\n",
    "    ax.hist(u, bins=100, log=True, histtype='step', label=f'{size} log rand choice')\n",
    "\n",
    "    # Obtain histogram data to define bins\n",
    "    h, b = np.histogram(x, bins=100)\n",
    "\n",
    "    # Calculate bin indices and probabilities\n",
    "    bin_indices = np.digitize(x, bins=b[:-1], right=True) - 1\n",
    "    raw_probs = 1.0 / np.take(h, bin_indices, mode='clip')\n",
    "    probs = raw_probs / raw_probs.sum()\n",
    "\n",
    "\n",
    "    z = np.random.choice(x, size=size, replace=True, p=probs)  # Allow replacement since probabilities are used\n",
    "    ax.hist(z, bins=100, log=True, histtype='step', label=f'{size} unif rand choice')\n",
    "\n",
    "    ax.set_ylabel('Frequency (log scale)')\n",
    "    ax.set_xlabel('Incident energies [GeV]')\n",
    "    ax.legend(fontsize= 20)\n",
    "utils.free_memory()\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # rect adjusts the space for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_samples = len(showers)\n",
    "num_samples = 10000 # choose the number of samples in the training dataset here\n",
    "\n",
    "def get_percentage(total, part):\n",
    "        return part / total\n",
    "\n",
    "data_percentage = get_percentage(tot_samples, num_samples)\n",
    "if num_samples != tot_samples : \n",
    "    \n",
    "    print(f'the data percentage is: \\n {(data_percentage*100):.4f} % of total {tot_samples}')\n",
    "else:\n",
    "    print('the total training dataset is considered')\n",
    "\n",
    "if data_percentage < 1. and data_percentage > 0.:\n",
    "    size = int ( tot_samples * data_percentage )\n",
    "    idx = np.random.choice(tot_samples, size=size, replace=False)\n",
    "    idx = np.sort(idx).astype(int)\n",
    "    print(f'{(data_percentage*100):.2f} % of the dataset is considered!')\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming utils.read_hdf5_file2 is properly defined and provides correct data.\n",
    "path_training = '/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/100_train_dset1-2_prep_0-500GeV.hdf5'\n",
    "_, impact_energies, showers = utils.read_hdf5_file2(path_training)\n",
    "impact_energies_numpy = np.array(impact_energies).ravel()\n",
    "\n",
    "sampling_sizes = [1000, 10000]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(sampling_sizes), figsize=(18, 6), sharex=True, sharey=True)\n",
    "fig.suptitle('Analysis of Incident Energies with Different Sampling Sizes', fontsize=26)\n",
    "\n",
    "# Iterate over each sampling size and subplot\n",
    "for ax, size in zip(axes, sampling_sizes):\n",
    "    x = impact_energies_numpy.copy()\n",
    "    u_indices = np.random.choice(len(x), size=size, replace=False)\n",
    "    u = x[u_indices]\n",
    "    \n",
    "    ax.hist(x, bins=100, log=True, histtype='step', label='Original')\n",
    "    ax.hist(u, bins=100, log=True, histtype='step', label=f'{size} log rand choice')\n",
    "\n",
    "    # Obtain histogram data to define bins\n",
    "    h, b = np.histogram(x, bins=100)\n",
    "\n",
    "    # Calculate bin indices and probabilities\n",
    "    bin_indices = np.digitize(x, bins=b[:-1], right=True) - 1\n",
    "    raw_probs = 1.0 / np.take(h, bin_indices, mode='clip')\n",
    "    probs = raw_probs / raw_probs.sum()\n",
    "\n",
    "    z_indices = np.random.choice(len(x), size=size, replace=True, p=probs)\n",
    "    z_indices = np.sort(z_indices).astype(int)\n",
    "    z = x[z_indices]\n",
    "    ax.hist(z, bins=100, log=True, histtype='step', label=f'{size} unif rand choice')\n",
    "\n",
    "    ax.set_ylabel('Frequency (log scale)')\n",
    "    ax.set_xlabel('Incident energies [GeV]')\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    # Use the indices to select corresponding showers\n",
    "    selected_showers_u = showers[u_indices]\n",
    "    selected_showers_z = showers[z_indices]\n",
    "\n",
    "utils.free_memory()\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # rect adjusts the space for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {\n",
    "        'energy': impact_energies[z_indices],\n",
    "        'events': showers[z_indices],\n",
    "    }\n",
    "print(f\"Shape of preprocessed data: {train_data_dict['events'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the files\n",
    "train_file='uniform_train_dset1-2_prep_0-500GeV.hdf5'\n",
    "path = \"/gpfs/dust/maxwell/user/valentel/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/\"\n",
    "\n",
    "file_path = path + '10k' + '_' + train_file\n",
    "utils.save_hdf5(train_data_dict, file_path= file_path )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calo-transfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
