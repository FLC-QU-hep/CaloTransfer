{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import utils.plot_finals as finals\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "        # Use a serif font that's likely available\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['DejaVu Serif', 'Liberation Serif', 'Computer Modern Roman', 'Bitstream Vera Serif'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 12,\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 600,  # Higher DPI for publication quality\n",
    "        'savefig.format': 'pdf',  # PDF format is often preferred for publications\n",
    "        'savefig.bbox': 'tight',\n",
    "        'savefig.pad_inches': 0.1,\n",
    "        'axes.linewidth': 0.8,  # Slightly thinner axes lines\n",
    "        'lines.linewidth': 1.5,  # Slightly thicker plot lines\n",
    "        'lines.markersize': 4,  # Slightly smaller markers\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointWise net + Showerflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza la lista con 10k\n",
    "training_steps=(\n",
    "    10000,\n",
    "    50000,\n",
    "    100000,\n",
    "    150000, \n",
    "    200000, \n",
    "    250000,\n",
    "    500000,\n",
    "    750000, \n",
    "    1000000, \n",
    ")\n",
    "print(training_steps)\n",
    "print(\"len(training_steps):\", len(training_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_step = training_steps  # or use: np.array([...]).astype(int)\n",
    "\n",
    "# Base configuration paths\n",
    "BASE_DIR = '/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/results/diffusion/'\n",
    "\n",
    "# Define checkpoint patterns - only the parts that differ\n",
    "CHECKPOINT_PATTERNS = {\n",
    "    ###### 10-90 GeV ######\n",
    "    # Vanilla models\n",
    "    'vanilla':    '2025_02_04__18_52_56',\n",
    "    'vanilla_v1': '2025_03_14__12_48_20',\n",
    "    'vanilla_v2': '2025_03_13__10_48_14',\n",
    "    'vanilla_v3': '2025_03_14__05_16_47',\n",
    "    'vanilla_v4': '2025_03_29__11_12_02',\n",
    "    'vanilla_v5': '2025_03_30__13_17_42',\n",
    "    'vanilla_v6': '2025_03_31__14_22_22',\n",
    "    \n",
    "    # Full finetune models\n",
    "    'full_finetune':    '2025_02_04__18_15_56',\n",
    "    'full_finetune_v1': '2025_03_16__11_00_43',\n",
    "    'full_finetune_v2': '2025_03_17__13_49_51',\n",
    "    'full_finetune_v3': '2025_03_18__16_36_36',\n",
    "    \n",
    "    # 3-layer frozen finetune\n",
    "    '3frozen_finetune_v6': '2025_03_09__19_00_21',\n",
    "    '3frozen_finetune_v7': '2025_03_10__18_20_44',\n",
    "    '3frozen_finetune_v8': '2025_03_11__17_41_36',\n",
    "    \n",
    "    # LoRA models\n",
    "    'lora_full_v1': '2025_03_19__23_39_27',\n",
    "    'lora_full_v2': '2025_03_21__10_53_31',\n",
    "    'lora_full_v3': '2025_03_23__13_07_39',\n",
    "    \n",
    "    ###### 1-1000 GeV ######\n",
    "    # 1-1000 GeV models\n",
    "    'vanilla_full_v1_1_1000': '2025_05_28__15_44_22',\n",
    "    'vanilla_full_v2_1_1000': '2025_05_08__16_28_38',\n",
    "    'vanilla_full_v3_1_1000': '2025_05_12__10_56_50',\n",
    "    'vanilla_full_v4_1_1000': '2025_08_18__09_32_53',\n",
    "    'vanilla_full_v5_1_1000': '2025_08_19__04_59_13',\n",
    "\n",
    "    'finetune_full_v1_1_1000': '2025_05_29__18_12_31',\n",
    "    'finetune_full_v2_1_1000': '2025_05_09__16_58_31',\n",
    "    'finetune_full_v3_1_1000': '2025_05_26__11_35_26',\n",
    "    'finetune_full_v4_1_1000': '2025_08_20__16_24_57',\n",
    "    'finetune_full_v5_1_1000': '2025_08_21__11_49_02',    \n",
    "    # BitFit models\n",
    "    'finetune_bitfit_v1_1_1000': '2025_06_12__13_09_43',\n",
    "    'finetune_bitfit_v2_1_1000': '2025_06_13__13_56_59',\n",
    "    'finetune_bitfit_v3_1_1000': '2025_06_14__12_59_27',\n",
    "    'finetune_bitfit_v4_1_1000': '2025_08_25__09_50_18',\n",
    "    'finetune_bitfit_v5_1_1000': '2025_08_26__01_21_48',\n",
    "\n",
    "    # Top-3 layers\n",
    "    'finetune_top3_v1_1_1000': '2025_06_18__16_15_13',\n",
    "    'finetune_top3_v2_1_1000': '2025_07_22__20_32_56',\n",
    "    'finetune_top3_v3_1_1000': '2025_07_23__14_50_23',\n",
    "    'finetune_top3_v4_1_1000': '2025_08_22__10_03_26',\n",
    "    'finetune_top3_v5_1_1000': '2025_08_23__03_00_04',\n",
    "\n",
    "    # LoRA\n",
    "    'lora_r1_v1_1_1000': '2025_07_21__19_20_51',\n",
    "    'lora_r2_v1_1_1000': '2025_07_18__17_45_10',\n",
    "    'lora_r4_v1_1_1000': '2025_07_19__04_03_45',\n",
    "    \n",
    "    'lora_r8_v1_1_1000': '2025_06_24__16_10_32',\n",
    "    'lora_r8_v2_1_1000': '2025_07_24__17_42_07',  # Same timestamp as lora_r8_v1\n",
    "    'lora_r8_v3_1_1000': '2025_07_25__12_25_39',  # Same timestamp as lora_r8_v1 and v2\n",
    "\n",
    "    'lora_r16_v1_1_1000': '2025_06_26__13_29_31',\n",
    "    'lora_r32_v1_1_1000': '2025_07_17__02_02_46',\n",
    "    'lora_r32a48_v1_1_1000': '2025_07_17__02_06_49',\n",
    "    'lora_r48_v1_1_1000': '2025_07_15__20_55_42',\n",
    "    'lora_r64_v1_1_1000': '2025_07_15__09_42_59',\n",
    "    \n",
    "    'lora_r106_v1_1_1000': '2025_07_25__14_36_14',  # New model with r=106, alpha=106\n",
    "    'lora_r106_v2_1_1000': '2025_07_29__10_10_18',  # Same timestamp as lora_r106_v1\n",
    "    'lora_r106_v3_1_1000': '2025_07_30__13_50_31',  # Same timestamp as lora_r106_v1 and v2\n",
    "    'lora_r106_v4_1_1000': '2025_08_28__11_56_05',  # Same timestamp as lora_r106_v1 and v2\n",
    "    'lora_r106_v5_1_1000': '2025_08_29__15_01_22',  # Same timestamp as lora_r106_v1 and v2\n",
    "\n",
    "    'lora_r204_v1_1_1000': '2025_07_26__11_44_31',\n",
    "}\n",
    "\n",
    "# Special cases that don't follow standard patterns\n",
    "SPECIAL_CONFIGS = {\n",
    "    # Models with different base directories\n",
    "    '3frozen_finetune_v6': {\n",
    "        'base_dir': BASE_DIR + '11_03_finetune_3layers_v6/finetune_3layers_v6/',\n",
    "        'for_loading': 'finetune_3layers_v6'\n",
    "    },\n",
    "    '3frozen_finetune_v7': {\n",
    "        'base_dir': BASE_DIR + '11_03_finetune_3layers_v7/finetune_3layers_v7/',\n",
    "        'for_loading': 'finetune_3layers_v7'\n",
    "    },\n",
    "    '3frozen_finetune_v8': {\n",
    "        'base_dir': BASE_DIR + '11_03_finetune_3layers_v8/finetune_3layers_v8/',\n",
    "        'for_loading': 'finetune_3layers_v8'\n",
    "    },\n",
    "    'finetune_top3_v1_1_1000': {\n",
    "        'base_dir': BASE_DIR + 'finetune_top3_v1_1_1000/',  # Fixed: using correct directory\n",
    "        'for_loading': 'finetune_top3_v1_1_1000'\n",
    "    },\n",
    "    # Models with different filename suffixes\n",
    "    'full_finetune': {\n",
    "        'suffix': '_pretrained',\n",
    "    },\n",
    "\n",
    "    'lora': {\n",
    "        'suffix': '_pretrained_lora',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Models that use pretrained suffix\n",
    "PRETRAINED_MODELS = [\n",
    "    # === 10-90 GeV Pretrained Models ===\n",
    "    'full_finetune', 'full_finetune_v1', 'full_finetune_v2', 'full_finetune_v3',\n",
    "    # === 1-1000 GeV Pretrained Models ===\n",
    "    'finetune_full_v1_1_1000', 'finetune_full_v2_1_1000', 'finetune_full_v3_1_1000', 'finetune_full_v4_1_1000', 'finetune_full_v5_1_1000',\n",
    "    'finetune_bitfit_v1_1_1000', 'finetune_bitfit_v2_1_1000', 'finetune_bitfit_v3_1_1000', 'finetune_bitfit_v4_1_1000', 'finetune_bitfit_v5_1_1000',\n",
    "    'finetune_top3_v1_1_1000', 'finetune_top3_v2_1_1000', 'finetune_top3_v3_1_1000', 'finetune_top3_v4_1_1000', 'finetune_top3_v5_1_1000',\n",
    "]\n",
    "\n",
    "# Models that use lora suffix\n",
    "LORA_MODELS = ['lora_full_v1', 'lora_full_v2', 'lora_full_v3', \n",
    "                'lora_r1_v1_1_1000','lora_r2_v1_1_1000','lora_r4_v1_1_1000',\n",
    "                'lora_r8_v1_1_1000', 'lora_r8_v2_1_1000', 'lora_r8_v3_1_1000',\n",
    "                'lora_r16_v1_1_1000', 'lora_r32_v1_1_1000', 'lora_r32a48_v1_1_1000','lora_r48_v1_1_1000',\n",
    "                'lora_r64_v1_1_1000', \n",
    "                'lora_r106_v1_1_1000', 'lora_r106_v2_1_1000', 'lora_r106_v3_1_1000', 'lora_r106_v4_1_1000', 'lora_r106_v5_1_1000',\n",
    "                'lora_r204_v1_1_1000', ]\n",
    "\n",
    "# Active strategies (uncomment to enable)\n",
    "ACTIVE_STRATEGIES = [\n",
    "    # === 1-1000GeV Active Models ===\n",
    "    'vanilla_full_v1_1_1000',\n",
    "    'vanilla_full_v2_1_1000',\n",
    "    'vanilla_full_v3_1_1000',\n",
    "    'vanilla_full_v4_1_1000',\n",
    "    'vanilla_full_v5_1_1000',\n",
    "\n",
    "    'finetune_full_v1_1_1000',\n",
    "    'finetune_full_v2_1_1000',\n",
    "    'finetune_full_v3_1_1000',\n",
    "    'finetune_full_v4_1_1000',\n",
    "    'finetune_full_v5_1_1000',\n",
    "    \n",
    "    'finetune_bitfit_v1_1_1000',\n",
    "    'finetune_bitfit_v2_1_1000',\n",
    "    'finetune_bitfit_v3_1_1000',\n",
    "    'finetune_bitfit_v4_1_1000',\n",
    "    'finetune_bitfit_v5_1_1000',\n",
    "\n",
    "    'finetune_top3_v1_1_1000',\n",
    "    'finetune_top3_v2_1_1000',\n",
    "    'finetune_top3_v3_1_1000',\n",
    "    'finetune_top3_v4_1_1000',\n",
    "    'finetune_top3_v5_1_1000',\n",
    "\n",
    "\n",
    "    \n",
    "    # 'lora_r1_v1_1_1000',\n",
    "    # 'lora_r2_v1_1_1000',\n",
    "    # 'lora_r4_v1_1_1000',\n",
    "\n",
    "    # 'lora_r8_v1_1_1000',\n",
    "    # 'lora_r8_v2_1_1000',\n",
    "    # 'lora_r8_v3_1_1000',\n",
    "\n",
    "    # 'lora_r16_v1_1_1000',\n",
    "    # 'lora_r32_v1_1_1000',\n",
    "    # # 'lora_r32a48_v1_1_1000',\n",
    "    # 'lora_r48_v1_1_1000',\n",
    "    # 'lora_r64_v1_1_1000',\n",
    "\n",
    "    'lora_r106_v1_1_1000', \n",
    "    'lora_r106_v2_1_1000',  # Same timestamp as lora_r106_v1\n",
    "    'lora_r106_v3_1_1000',  # Same timestamp as lora_r106_v1 and v2\n",
    "    'lora_r106_v4_1_1000',  # Same timestamp as lora_r106_v1 and v2\n",
    "    'lora_r106_v5_1_1000',  # Same timestamp as lora_r106_v1 and v2\n",
    "\n",
    "    # 'lora_r204_v1_1_1000',\n",
    "]\n",
    "\n",
    "\n",
    "def build_strategy_config(strategy_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Build configuration for a given strategy based on patterns and special cases.\"\"\"\n",
    "    \n",
    "    # Get timestamp pattern\n",
    "    if strategy_name not in CHECKPOINT_PATTERNS:\n",
    "        raise ValueError(f\"Strategy {strategy_name} not found in CHECKPOINT_PATTERNS\")\n",
    "    \n",
    "    timestamp = CHECKPOINT_PATTERNS[strategy_name]\n",
    "    \n",
    "    # Get special configurations if any\n",
    "    special = SPECIAL_CONFIGS.get(strategy_name, {})\n",
    "    \n",
    "    # Build base configuration\n",
    "    config = {\n",
    "        'for_loading': special.get('for_loading', strategy_name),\n",
    "        'base_dir': special.get('base_dir', BASE_DIR + strategy_name + '/'),\n",
    "        'wd_per_epoch': {},\n",
    "        'kl_per_epoch': {}\n",
    "    }\n",
    "    \n",
    "    # Build checkpoint filename\n",
    "    suffix = ''\n",
    "    if strategy_name in PRETRAINED_MODELS:\n",
    "        suffix = '_pretrained'\n",
    "    elif strategy_name in LORA_MODELS:\n",
    "        suffix = '_pretrained_lora'\n",
    "    \n",
    "    config['ckpt_filename'] = f'CaloChallange_CD{timestamp}_ckpt_0.000000_{{step}}.pt{suffix}'\n",
    "    \n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final configuration dictionary\n",
    "strategy_configs = {\n",
    "    strategy: build_strategy_config(strategy) \n",
    "    for strategy in ACTIVE_STRATEGIES\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"Loading data for {len(strategy_configs)} strategies...\")\n",
    "\n",
    "# Track loading statistics\n",
    "loading_stats = {\n",
    "    'total_attempts': 0,\n",
    "    'wd_loaded': 0,\n",
    "    'kl_loaded': 0,\n",
    "    'failed_strategies': set(),\n",
    "    'partially_failed': {}\n",
    "}\n",
    "\n",
    "# Loop over each strategy and training step to process checkpoints\n",
    "for strategy, config in strategy_configs.items():\n",
    "    strategy_failures = []\n",
    "    \n",
    "    for step in grad_step:\n",
    "        loading_stats['total_attempts'] += 1\n",
    "        \n",
    "        # Build checkpoint path using the format string\n",
    "        ckpt_filename = config['ckpt_filename'].format(step=step)\n",
    "        ckpt_path = os.path.join(config['base_dir'], ckpt_filename)\n",
    "        \n",
    "        # Load Wasserstein distance using a custom function (assumed to be defined)\n",
    "        wasserstein_dist = finals.load_metric(output_dir=ckpt_path, strategy=strategy, metric_name='wasserstein_dist')\n",
    "        quantile_kl = finals.load_metric(output_dir=ckpt_path, strategy=strategy, metric_name='kl_divergences')\n",
    "        \n",
    "        # Track failures\n",
    "        failures = []\n",
    "        if wasserstein_dist is None:\n",
    "            failures.append('WD')\n",
    "        else:\n",
    "            config['wd_per_epoch'][step] = wasserstein_dist\n",
    "            loading_stats['wd_loaded'] += 1\n",
    "            \n",
    "        if quantile_kl is None:\n",
    "            failures.append('KL')\n",
    "        else:\n",
    "            config['kl_per_epoch'][step] = quantile_kl\n",
    "            loading_stats['kl_loaded'] += 1\n",
    "        \n",
    "        # Only print if something failed\n",
    "        if failures:\n",
    "            strategy_failures.append((step, failures))\n",
    "    \n",
    "    # Report strategy-level failures\n",
    "    if strategy_failures:\n",
    "        if len(strategy_failures) == len(grad_step):\n",
    "            # Complete failure for this strategy\n",
    "            loading_stats['failed_strategies'].add(strategy)\n",
    "            print(f\"\\n❌ {strategy}: Failed to load any data\")\n",
    "        else:\n",
    "            # Partial failure\n",
    "            loading_stats['partially_failed'][strategy] = strategy_failures\n",
    "            print(f\"\\n⚠️  {strategy}: Failed {len(strategy_failures)}/{len(grad_step)} steps\")\n",
    "            for step, failures in strategy_failures[:5]:  # Show first 5 failures\n",
    "                print(f\"   Step {step}: {', '.join(failures)} missing\")\n",
    "            if len(strategy_failures) > 5:\n",
    "                print(f\"   ... and {len(strategy_failures) - 5} more failures\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING SUMMARY:\")\n",
    "print(f\"Total attempts: {loading_stats['total_attempts']}\")\n",
    "print(f\"Successful loads: WD={loading_stats['wd_loaded']}, KL={loading_stats['kl_loaded']}\")\n",
    "print(f\"Completely failed strategies: {len(loading_stats['failed_strategies'])}\")\n",
    "print(f\"Partially failed strategies: {len(loading_stats['partially_failed'])}\")\n",
    "\n",
    "if loading_stats['failed_strategies']:\n",
    "    print(f\"\\nStrategies with no data: {', '.join(loading_stats['failed_strategies'])}\")\n",
    "\n",
    "# Show successfully loaded strategies count\n",
    "successful_strategies = []\n",
    "for strategy, config in strategy_configs.items():\n",
    "    if strategy not in loading_stats['failed_strategies'] and config['wd_per_epoch']:\n",
    "        successful_strategies.append(strategy)\n",
    "\n",
    "if successful_strategies:\n",
    "    print(f\"\\n✓ Successfully loaded data for {len(successful_strategies)} strategies\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop over each strategy configuration to plot its Wasserstein distances\n",
    "# for strategy, config in strategy_configs.items():\n",
    "#     # Create a subtitle by formatting the strategy name (customize if needed)\n",
    "#     subtitle = strategy.replace(\"_\", \" \").title()\n",
    "    \n",
    "#     fig = finals.plot_wasserstein_distances_features(\n",
    "#         config['wd_per_epoch'],\n",
    "#         title='WD_Features_EWC_every1k_epoch',\n",
    "#         save_plots=False,\n",
    "#         show_legend=True,\n",
    "#         subtitle=subtitle,\n",
    "#         figsize=(20, 20),\n",
    "#     )\n",
    "\n",
    "#     fig = finals.plot_wasserstein_distances_features(\n",
    "#         config['kl_per_epoch'],\n",
    "#         title='KL_Features_EWC_every1k_epoch',\n",
    "#         save_plots=False,\n",
    "#         show_legend=True,\n",
    "#         subtitle=subtitle,\n",
    "#         figsize=(20, 20),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically extract the metrics dictionaries and labels from strategy_configs\n",
    "wd_dicts = []\n",
    "kl_dicts = []\n",
    "\n",
    "labels = []\n",
    "labels_kl = []\n",
    "\n",
    "for strategy, config in strategy_configs.items():\n",
    "    wd_dicts.append(config['wd_per_epoch'])\n",
    "    labels.append(config.get('label', strategy.replace('_', ' ').title()))\n",
    "\n",
    "    kl_dicts.append(config['kl_per_epoch'])\n",
    "    labels_kl.append(config.get('label', strategy.replace('_', ' ').title()))\n",
    "\n",
    "# Call the analysis function with the unpacked list of dictionaries\n",
    "results_wd = finals.analyze_and_plot_metrics(\n",
    "    *wd_dicts,\n",
    "    labels=labels,\n",
    "    avg_method='geometric'  # Use geometric mean for KL divergence as well\n",
    ")\n",
    "results_kl = finals.analyze_and_plot_metrics(\n",
    "    *kl_dicts,\n",
    "    labels=labels_kl,\n",
    "    avg_method='geometric'  # Use geometric mean for KL divergence as well\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\n",
    "    'Voxel Energy Spectrum',\n",
    "    'Energy Ratio',\n",
    "    'Visible Energy',\n",
    "    'Occupancy',\n",
    "    'Longitudinal Profile',\n",
    "    'Radial Profile'\n",
    "]\n",
    "\n",
    "config_names = list(strategy_configs.keys())\n",
    "\n",
    "# For Wasserstein\n",
    "reorganized_data_wd = reorganized_data_wd = finals.reorganize_metrics(config_names, results_wd, \n",
    "                                                                    metric_names, 'Wasserstein Distances')\n",
    "\n",
    "# For KL\n",
    "reorganized_data_kl = finals.reorganize_metrics(config_names, results_kl, \n",
    "                                                metric_names, 'KL Divergences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finals plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_groups = {\n",
    "    # \"From Scratch\": [\"vanilla_v4_w5k_10-90\", \"vanilla_v5_w5k_10-90\", \"vanilla_v6_w5k_10-90\"],\n",
    "    # \"Full Finetuned\": [\"finetune_full_v1_w5k_10-90\", \"finetune_full_v2_w5k_10-90\", \"finetune_full_v3_w5k_10-90\"],\n",
    "   \n",
    "    # \"From Scratch\": [\"vanilla\"],\n",
    "    # \"Full Finetuned\": [\"full_finetune\"],\n",
    "    # \"Top Layers Finetuned\": [\"3frozen_finetune\"],\n",
    "    # \"LoRA (16)\": [\"lora_full_v1\", \"lora_full_v2\"],\n",
    "    # \"LoRA (4)\": [\"lora_full_v3\"],\n",
    "\n",
    "    # == 1_1000GeV ===\n",
    "    \"From scratch\": [\n",
    "        \"vanilla_full_v1_1_1000\",\n",
    "        \"vanilla_full_v2_1_1000\", \n",
    "        \"vanilla_full_v3_1_1000\",\n",
    "        \"vanilla_full_v4_1_1000\",\n",
    "        \"vanilla_full_v5_1_1000\",\n",
    "    ],\n",
    "    \"Full fine-tuned\": [\n",
    "        \"finetune_full_v1_1_1000\",\n",
    "        \"finetune_full_v2_1_1000\", \n",
    "        \"finetune_full_v3_1_1000\",\n",
    "        \"finetune_full_v4_1_1000\",\n",
    "        \"finetune_full_v5_1_1000\",\n",
    "    ],\n",
    "    \"BitFit\": [\n",
    "        \"finetune_bitfit_v1_1_1000\",\n",
    "        \"finetune_bitfit_v2_1_1000\",\n",
    "        \"finetune_bitfit_v3_1_1000\",\n",
    "        \"finetune_bitfit_v4_1_1000\",\n",
    "        \"finetune_bitfit_v5_1_1000\",\n",
    "    ],\n",
    "\n",
    "    \"Top2\": [\"finetune_top3_v1_1_1000\",\n",
    "             \"finetune_top3_v2_1_1000\", \n",
    "             \"finetune_top3_v3_1_1000\",\n",
    "             \"finetune_top3_v4_1_1000\",\n",
    "             \"finetune_top3_v5_1_1000\",\n",
    "             ],\n",
    "\n",
    "    # \"LoRA (r=1)\": [\"lora_r1_v1_1_1000\"],\n",
    "    # \"LoRA (r=2)\": [\"lora_r2_v1_1_1000\"],\n",
    "    # \"LoRA (r=4)\": [\"lora_r4_v1_1_1000\"],\n",
    "    # \"LoRA (r=8)\": [\"lora_r8_v1_1_1000\",\n",
    "    #                \"lora_r8_v2_1_1000\", \n",
    "    #                \"lora_r8_v3_1_1000\"\n",
    "    #                ],\n",
    "    # \"LoRA (r=16)\": [\"lora_r16_v1_1_1000\"],\n",
    "    # \"LoRA (r=32)\": [\"lora_r32_v1_1_1000\"],\n",
    "    # # \"LoRA (r=32)\": [\"lora_r32a48_v1_1_1000\"],\n",
    "    # \"LoRA (r=48)\": [\"lora_r48_v1_1_1000\"],\n",
    "    # \"LoRA (r=64)\": [\"lora_r64_v1_1_1000\"],\n",
    "\n",
    "    \"LoRA R106\": [\"lora_r106_v1_1_1000\",\n",
    "                    \"lora_r106_v2_1_1000\", \n",
    "                    \"lora_r106_v3_1_1000\",\n",
    "                    \"lora_r106_v4_1_1000\",\n",
    "                    \"lora_r106_v5_1_1000\",\n",
    "                    ],\n",
    "    # \"LoRA (r=204)\": [\"lora_r204_v1_1_1000\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = 1.05e5  # Set x-axis limit to 1M\n",
    "title = 'Incident Energy: 1 - 1000 GeV'\n",
    "\n",
    "# 1) Define your selected_group as a list of dicts,\n",
    "#    using the Okabe–Ito color‐blind–safe palette for publications:\n",
    "from matplotlib.colors import to_hex\n",
    "\n",
    "# Generate plasma colors for LoRA models\n",
    "lora_colors = [to_hex(c) for c in plt.cm.plasma(np.linspace(0.1, 0.9, 8))]\n",
    "\n",
    "selected_group = [\n",
    "    {\"name\": \"From scratch\", \"color\": \"#0D3B66\"},\n",
    "    {\"name\": \"Full fine-tuned\", \"color\": \"#C03221\"},\n",
    "    {\"name\": \"BitFit\", \"color\": \"#00A676\"},\n",
    "    {\"name\": \"Top2\", \"color\": \"#995FA3\"},\n",
    "    \n",
    "    {\"name\": \"LoRA R106\", \"color\": \"#393A10\"},  # New model with r=106, alpha=106\n",
    "]\n",
    "\n",
    "# 2) Immediately build the two parallel lists your function needs:\n",
    "names  = [g[\"name\"]  for g in selected_group]\n",
    "colors = [g[\"color\"] for g in selected_group]\n",
    "\n",
    "# 3) Call exactly as before, swapping in 'names' & 'colors':\n",
    "finals.plot_reorganized_data(\n",
    "    reorganized_data_wd,\n",
    "    strategy_configs,\n",
    "    # main_title=title,\n",
    "    ylabel='Normalized WD',\n",
    "    save_plot=False,\n",
    "    filename='for_paper/finals_1-1000GeV_normalised_wd.pdf',\n",
    "    x_max=x_max,\n",
    "    group_prefixes=custom_groups,\n",
    "    selected_group=names,   # list of group keys\n",
    "    colors=colors,          # matching, colorblind‐safe list\n",
    "    use_weighted_average=True,\n",
    "    weight_method='Geometric',\n",
    ")\n",
    "\n",
    "finals.plot_reorganized_data(\n",
    "    reorganized_data_kl,\n",
    "    strategy_configs,\n",
    "    # main_title=title,\n",
    "    ylabel='KLD',\n",
    "    save_plot=False,\n",
    "    filename='for_paper/finals_1-1000GeV_quantile_kl.pdf',\n",
    "    x_max=x_max,\n",
    "    group_prefixes=custom_groups,\n",
    "    selected_group=names,\n",
    "    colors=colors,\n",
    "    use_weighted_average=True,\n",
    "    weight_method='Geometric',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_group = [\n",
    "#     {\"name\": \"From Scratch\", \"params\": \"524.0K (100.0%)\"},\n",
    "#     {\"name\": \"Full Finetuned\", \"params\": \"524.0K (100.0%)\"},  \n",
    "#     {\"name\": \"BitFit\", \"params\": \"87.38K (17.30%)\"},             \n",
    "#     {\"name\": \"Top3\", \"params\": \"220.50K (43.67%)\"},             \n",
    "\n",
    "#     # {\"name\": \"LoRA (r=1)\", \"params\": \"2.67K (0.51%)\"},          \n",
    "#     # {\"name\": \"LoRA (r=2)\", \"params\": \"5.14K (1.01%)\"},    \n",
    "#     # {\"name\": \"LoRA (r=4)\", \"params\": \"10.27K (1.99%)\"},                \n",
    "#     {\"name\": \"LoRA (r=8)\", \"params\": \"20.54K (3.91%)\"},          \n",
    "#     # {\"name\": \"LoRA (r=16)\", \"params\": \"41.1K (7.53%)\"},        \n",
    "#     # {\"name\": \"LoRA (r=32)\", \"params\": \"82.18K (14.00%)\"},        \n",
    "#     # {\"name\": \"LoRA (r=48)\", \"params\": \"123.26K (19.62%)\"},        \n",
    "#     # {\"name\": \"LoRA (r=64)\", \"params\": \"164.35K (24.56%)\"},        \n",
    "# ]\n",
    "\n",
    "selected_group = [\n",
    "    {\"name\": \"From Scratch\", \"params\": \"504.92K \"},\n",
    "    {\"name\": \"Full Finetuned\", \"params\": \"504.92K \"},  \n",
    "    # {\"name\": \"BitFit\", \"params\": \"87.38K \"},             \n",
    "    # {\"name\": \"Top3\", \"params\": \"220.50K \"},             \n",
    "\n",
    "    # {\"name\": \"LoRA (r=1)\", \"params\": \"2.67K\"},          \n",
    "    # {\"name\": \"LoRA (r=2)\", \"params\": \"5.14K\"},    \n",
    "    # {\"name\": \"LoRA (r=4)\", \"params\": \"10.27K\"},                \n",
    "    {\"name\": \"LoRA (r=8)\", \"params\": \"20.54K\"},          \n",
    "    # {\"name\": \"LoRA (r=16)\", \"params\": \"41.1K\"},        \n",
    "    # {\"name\": \"LoRA (r=32)\", \"params\": \"82.18K \"},        \n",
    "    # {\"name\": \"LoRA (r=48)\", \"params\": \"123.26K \"},        \n",
    "    # {\"name\": \"LoRA (r=64)\", \"params\": \"164.35K \"},     \n",
    "    {\"name\": \"LoRA (r=106)\", \"params\": \"272,21K \"},     \n",
    "    # {\"name\": \"LoRA (r=204)\", \"params\": \"523.87K \"},     \n",
    "]\n",
    "\n",
    "names = [g[\"name\"] for g in selected_group]\n",
    "\n",
    "# Create a mapping from name to params for the table\n",
    "name_to_params = {g[\"name\"]: g[\"params\"] for g in selected_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [100, 1_000, 10_000, 100_000]\n",
    "# cols = [500, 5_000, 50_000]\n",
    "\n",
    "agg_mean, agg_std = finals.generate_overall_table(\n",
    "    reorganized_data_wd,\n",
    "    strategy_configs,\n",
    "    group_prefixes=custom_groups,   # <-- your dict of lists\n",
    "    selected_group=names,           # <-- the list of group names\n",
    "    cols = cols,\n",
    "    weight_method='Geometric',\n",
    "    metric_names=metric_names,\n",
    "    use_weighted_average=True\n",
    ")\n",
    "finals.print_overall_table(agg_mean, agg_std, name_to_params=name_to_params,\n",
    "                            save_latex=True,\n",
    "                            latex_path=\"results/for_paper/finals_1-1000GeV_overall_table_PEFT.tex\")\n",
    "\n",
    "fig, ax = finals.plot_performance_vs_params(\n",
    "    agg_mean, \n",
    "    agg_std, \n",
    "    name_to_params,\n",
    "    # colors=colors,          \n",
    "    metric_name=\"1 - Normalized Wasserstein Distance\",  # or \"Accuracy\" if higher is better\n",
    "    save_path=\"results/for_paper/performance_vs_params.pdf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now pass them in correctly:\n",
    "agg_mean, agg_std = finals.generate_overall_table(\n",
    "    reorganized_data_kl,\n",
    "    strategy_configs,\n",
    "    group_prefixes=custom_groups,   # <-- your dict of lists\n",
    "    selected_group=names,           # <-- the list of group names\n",
    "    cols = cols,\n",
    "    weight_method='Simple',\n",
    "    metric_names=metric_names,\n",
    "    use_weighted_average=True\n",
    ")\n",
    "\n",
    "finals.print_overall_table(agg_mean, agg_std, name_to_params=name_to_params,\n",
    "                            save_latex=True,\n",
    "                            latex_path=\"results/for_paper/finals_1-1000GeV_overall_table_PEFT_kl.tex\")\n",
    "# fig, ax = finals.plot_performance_vs_params(\n",
    "#     agg_mean, \n",
    "#     agg_std, \n",
    "#     name_to_params,\n",
    "#     # colors=colors,          \n",
    "\n",
    "#     metric_name=\"Kullback–Leibler divergence\",  # or \"Accuracy\" if higher is better\n",
    "#     # save_path=\"performance_vs_params.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pareto"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.makedirs(\"./results/for_paper/lora\", exist_ok=True)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Data from your tables (using average values)\n",
    "methods_data = {\n",
    "    'From Scratch': {'params': 504.92, 'wasserstein': 0.110, 'kl': 0.218, 'color': 'gray', 'marker': 'v'},\n",
    "    'Full FT': {'params': 504.92, 'wasserstein': 0.094, 'kl': 0.138, 'color': 'purple', 'marker': 's'},\n",
    "    'BitFit': {'params': 87.38, 'wasserstein': 0.103, 'kl': 0.146, 'color': 'green', 'marker': 's'},\n",
    "    'Top3': {'params': 220.50, 'wasserstein': 0.099, 'kl': 0.147, 'color': 'blue', 'marker': 's'},\n",
    "    'LoRA r=8': {'params': 20.54, 'wasserstein': 0.135, 'kl': 0.199, 'color': 'red', 'marker': 'o'},\n",
    "    'LoRA r=48': {'params': 123.26, 'wasserstein': 0.137, 'kl': 0.190, 'color': 'red', 'marker': 'o'},\n",
    "    'LoRA r=64': {'params': 164.35, 'wasserstein': 0.158, 'kl': 0.257, 'color': 'red', 'marker': 'o'},\n",
    "    'LoRA r=106': {'params': 300.35, 'wasserstein': 0.119, 'kl': 0.215, 'color': 'red', 'marker': 'o'},\n",
    "    'LoRA r=204': {'params': 523.87, 'wasserstein': 0.123, 'kl': np.nan, 'color': 'darkred', 'marker': 'o'},\n",
    "}\n",
    "\n",
    "# Subplot 1: Wasserstein Distance\n",
    "for method, data in methods_data.items():\n",
    "    if not np.isnan(data['wasserstein']):\n",
    "        if 'LoRA' in method:\n",
    "            ax1.scatter(data['params'], data['wasserstein'], s=200, c=data['color'], \n",
    "                       alpha=0.7, marker=data['marker'], edgecolors='black', linewidth=1.5)\n",
    "        else:\n",
    "            ax1.scatter(data['params'], data['wasserstein'], s=250, c=data['color'], \n",
    "                       alpha=0.8, marker=data['marker'], edgecolors='black', linewidth=2)\n",
    "        \n",
    "        # Annotations with better positioning\n",
    "        if method == 'LoRA r=204':\n",
    "            ax1.annotate(method, (data['params'], data['wasserstein']), \n",
    "                        xytext=(-80, 15), textcoords='offset points', fontsize=11, fontweight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow', alpha=0.7),\n",
    "                        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3'))\n",
    "        elif method == 'Full FT':\n",
    "            ax1.annotate(method, (data['params'], data['wasserstein']), \n",
    "                        xytext=(-80, -25), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "        elif method == 'From Scratch':\n",
    "            ax1.annotate(method, (data['params'], data['wasserstein']), \n",
    "                        xytext=(-100, 10), textcoords='offset points', fontsize=10)\n",
    "        else:\n",
    "            ax1.annotate(method.replace('LoRA ', ''), (data['params'], data['wasserstein']), \n",
    "                        xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add reference lines\n",
    "ax1.axvline(x=220.5, color='blue', linestyle='--', alpha=0.3, linewidth=1)\n",
    "ax1.axvline(x=523.87, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "# Highlight the paradox\n",
    "ax1.annotate('', xy=(523.87, 0.123), xytext=(220.5, 0.099),\n",
    "            arrowprops=dict(arrowstyle='<->', color='black', lw=2, alpha=0.5))\n",
    "ax1.text(370, 0.111, 'More params,\\nworse performance!', \n",
    "         ha='center', fontsize=12, fontweight='bold',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', edgecolor='red', linewidth=2))\n",
    "\n",
    "ax1.set_xlabel('Trainable Parameters (K)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Wasserstein Distance (↓ better)', fontsize=14, fontweight='bold')\n",
    "ax1.set_title('Parameter Efficiency: Wasserstein Distance', fontsize=16, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-50, 600)\n",
    "ax1.set_ylim(0.08, 0.17)\n",
    "\n",
    "# Subplot 2: KL Divergence\n",
    "for method, data in methods_data.items():\n",
    "    if not np.isnan(data.get('kl', np.nan)):\n",
    "        if 'LoRA' in method:\n",
    "            ax2.scatter(data['params'], data['kl'], s=200, c=data['color'], \n",
    "                       alpha=0.7, marker=data['marker'], edgecolors='black', linewidth=1.5)\n",
    "        else:\n",
    "            ax2.scatter(data['params'], data['kl'], s=250, c=data['color'], \n",
    "                       alpha=0.8, marker=data['marker'], edgecolors='black', linewidth=2)\n",
    "        \n",
    "        # Annotations\n",
    "        if method == 'Full FT':\n",
    "            ax2.annotate(method, (data['params'], data['kl']), \n",
    "                        xytext=(-80, -25), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "        elif method == 'From Scratch':\n",
    "            ax2.annotate(method, (data['params'], data['kl']), \n",
    "                        xytext=(-100, 10), textcoords='offset points', fontsize=10)\n",
    "        else:\n",
    "            ax2.annotate(method.replace('LoRA ', ''), (data['params'], data['kl']), \n",
    "                        xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax2.set_xlabel('Trainable Parameters (K)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('KL Divergence (↓ better)', fontsize=14, fontweight='bold')\n",
    "ax2.set_title('Parameter Efficiency: KL Divergence', fontsize=16, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(-50, 600)\n",
    "ax2.set_ylim(0.12, 0.27)\n",
    "\n",
    "# Main title\n",
    "# plt.suptitle('The LoRA Paradox: More Parameters, Worse Performance', fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='LoRA variants'),\n",
    "    Line2D([0], [0], marker='s', color='w', markerfacecolor='blue', markersize=10, label='Top3'),\n",
    "    Line2D([0], [0], marker='s', color='w', markerfacecolor='green', markersize=10, label='BitFit'),\n",
    "    Line2D([0], [0], marker='s', color='w', markerfacecolor='purple', markersize=10, label='Full fine-tuned'),\n",
    "    Line2D([0], [0], marker='v', color='w', markerfacecolor='gray', markersize=10, label='From scratch')\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/for_paper/lora/lora_paradox_performance_vs_params.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anlaysis weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import utils.finetune as ft\n",
    "import configs\n",
    "\n",
    "from models.epicVAE_nflows_kDiffusion import epicVAE_nFlow_kDiffusion\n",
    "\n",
    "def load_pretrained_model(cfg, checkpoint_path, use_ema=True):\n",
    "    \"\"\"Load pretrained model weights\"\"\"\n",
    "    model = epicVAE_nFlow_kDiffusion(cfg).to(cfg.device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=cfg.device, weights_only=False)\n",
    "    \n",
    "    if use_ema and 'others' in checkpoint and 'model_ema' in checkpoint['others']:\n",
    "        model.load_state_dict(checkpoint['others']['model_ema'], strict=True)\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_finetuned_models(cfg, base_path, use_ema=True):\n",
    "    \"\"\"Load all fine-tuned model variants\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Define checkpoint paths for each method\n",
    "    checkpoints = {\n",
    "        'Full FT': f'{base_path}/finetune/CaloChallange_CD2025_05_09__16_58_31/ckpt_0.000000_500000.pt',\n",
    "        'BitFit': f'{base_path}/finetune/CaloChallange_CD2025_06_12__13_09_43/ckpt_0.000000_500000.pt',\n",
    "        'Top3':   f'{base_path}/finetune/CaloChallange_CD2025_07_22__20_32_56/ckpt_0.000000_500000.pt',\n",
    "        \n",
    "        'LoRA r=204': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_26__11_44_31/ckpt_0.000000_50000.pt',\n",
    "        'LoRA r=106': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_25__14_36_14/ckpt_0.000000_50000.pt',\n",
    "\n",
    "        'LoRA r=64': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_15__09_42_59/ckpt_0.000000_50000.pt',\n",
    "\n",
    "        'LoRA r=48': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_15__20_55_42/ckpt_0.000000_50000.pt',\n",
    "        'LoRA r=16': f'{base_path}/finetune/lora/CaloChallange_CD2025_06_26__13_29_31/ckpt_0.000000_50000.pt',\n",
    "\n",
    "        'LoRA r=8': f'{base_path}/finetune/lora/CaloChallange_CD2025_06_24__16_10_32/ckpt_0.000000_50000.pt',\n",
    "        # 'LoRA r=8': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_24__13_39_58/ckpt_0.000000_140000.pt',\n",
    "        'LoRA r=4': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_19__04_03_45/ckpt_0.000000_50000.pt',\n",
    "        'LoRA r=2': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_18__17_45_10/ckpt_0.000000_50000.pt',\n",
    "        'LoRA r=1': f'{base_path}/finetune/lora/CaloChallange_CD2025_07_21__19_20_51/ckpt_0.000000_50000.pt',\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Load pretrained model first\n",
    "    pretrained_model = load_pretrained_model(cfg, cfg.diffusion_pretrained_model_path, use_ema)\n",
    "    \n",
    "    for method_name, ckpt_path in checkpoints.items():\n",
    "        if not Path(ckpt_path).exists():\n",
    "            print(f\"Checkpoint not found: {ckpt_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nLoading {method_name}...\")\n",
    "        \n",
    "        if 'LoRA' in method_name:\n",
    "            # Extract rank from method name\n",
    "            rank = int(method_name.split('=')[1])\n",
    "            \n",
    "            # Create model with LoRA architecture\n",
    "            model = epicVAE_nFlow_kDiffusion(cfg).to(cfg.device)\n",
    "            \n",
    "            # Load pretrained weights first\n",
    "            pretrained_ckpt = torch.load(cfg.diffusion_pretrained_model_path, map_location=cfg.device, weights_only=False)\n",
    "            if use_ema:\n",
    "                model.load_state_dict(pretrained_ckpt['others']['model_ema'], strict=True)\n",
    "            else:\n",
    "                model.load_state_dict(pretrained_ckpt['state_dict'], strict=True)\n",
    "            \n",
    "            # Apply LoRA architecture\n",
    "            for i in range(6):  # Assuming 6 layers\n",
    "                layer = model.diffusion.inner_model.layers[i]\n",
    "                model.diffusion.inner_model.layers[i] = ft.apply_lora(layer, rank=rank, alpha=rank)\n",
    "            \n",
    "            # Load LoRA weights\n",
    "            lora_ckpt = torch.load(ckpt_path, map_location=cfg.device, weights_only=False)\n",
    "            if use_ema:\n",
    "                lora_state = lora_ckpt['others']['model_ema']\n",
    "            else:\n",
    "                lora_state = lora_ckpt['state_dict']\n",
    "                \n",
    "            # Copy LoRA parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if '_layer.A' in name or '_layer.B' in name:\n",
    "                    if name in lora_state:\n",
    "                        param.data.copy_(lora_state[name])\n",
    "                        \n",
    "        else:\n",
    "            # Load regular fine-tuned model\n",
    "            model = epicVAE_nFlow_kDiffusion(cfg).to(cfg.device)\n",
    "            checkpoint = torch.load(ckpt_path, map_location=cfg.device, weights_only=False)\n",
    "            \n",
    "            if use_ema:\n",
    "                model.load_state_dict(checkpoint['others']['model_ema'], strict=True)\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "        \n",
    "        models[method_name] = model\n",
    "    \n",
    "    return pretrained_model, models\n",
    "\n",
    "def calculate_modification_magnitude(pretrained_model, finetuned_models, num_layers=6):\n",
    "    \"\"\"Calculate layer-wise modification magnitudes for each method\"\"\"\n",
    "    \n",
    "    modifications = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for method_name, model_ft in finetuned_models.items():\n",
    "            layer_mods = []\n",
    "            \n",
    "            for layer_idx in range(num_layers):\n",
    "                # Get pretrained layer\n",
    "                pretrained_layer = pretrained_model.diffusion.inner_model.layers[layer_idx]\n",
    "                finetuned_layer = model_ft.diffusion.inner_model.layers[layer_idx]\n",
    "                \n",
    "                if method_name == \"BitFit\":\n",
    "                    # Only bias changes\n",
    "                    if hasattr(pretrained_layer, '_layer'):\n",
    "                        bias_pre = pretrained_layer._layer.bias\n",
    "                        bias_ft = finetuned_layer._layer.bias\n",
    "                    else:\n",
    "                        bias_pre = pretrained_layer.bias\n",
    "                        bias_ft = finetuned_layer.bias\n",
    "                    \n",
    "                    if bias_pre is not None and bias_ft is not None:\n",
    "                        mod = torch.norm(bias_ft - bias_pre).item()\n",
    "                    else:\n",
    "                        mod = 0.0\n",
    "                        \n",
    "                elif \"LoRA\" in method_name:\n",
    "                    # LoRA modification magnitude\n",
    "                    if hasattr(finetuned_layer, '_layer') and hasattr(finetuned_layer._layer, 'A'):\n",
    "                        lora_layer = finetuned_layer._layer\n",
    "                        # lora_layer.alpha = 1  # Now we set alpha after lora_layer is defined\n",
    "                        # Calculate effective LoRA weight change\n",
    "                        lora_weight = (lora_layer.B @ lora_layer.A) * (lora_layer.alpha / lora_layer.rank)\n",
    "                        mod = torch.norm(lora_weight).item()\n",
    "                    else:\n",
    "                        mod = 0.0\n",
    "                        \n",
    "                elif method_name == \"Top3\":\n",
    "                    # Only last 3 layers are modified\n",
    "                    if layer_idx >= 3:\n",
    "                        if hasattr(pretrained_layer, '_layer'):\n",
    "                            W_pre = pretrained_layer._layer.weight\n",
    "                            W_ft = finetuned_layer._layer.weight\n",
    "                            bias_pre = pretrained_layer._layer.bias\n",
    "                            bias_ft = finetuned_layer._layer.bias\n",
    "                        else:\n",
    "                            W_pre = pretrained_layer.weight\n",
    "                            W_ft = finetuned_layer.weight\n",
    "                            bias_pre = pretrained_layer.bias\n",
    "                            bias_ft = finetuned_layer.bias\n",
    "                        \n",
    "                        # Combined weight and bias modification\n",
    "                        weight_mod = torch.norm(W_ft - W_pre).item()\n",
    "                        bias_mod = torch.norm(bias_ft - bias_pre).item() if bias_pre is not None else 0\n",
    "                        mod = weight_mod + bias_mod\n",
    "                    else:\n",
    "                        mod = 0.0\n",
    "                        \n",
    "                elif method_name == \"Full FT\":\n",
    "                    # All parameters changed\n",
    "                    if hasattr(pretrained_layer, '_layer'):\n",
    "                        W_pre = pretrained_layer._layer.weight\n",
    "                        W_ft = finetuned_layer._layer.weight\n",
    "                        bias_pre = pretrained_layer._layer.bias\n",
    "                        bias_ft = finetuned_layer._layer.bias\n",
    "                    else:\n",
    "                        W_pre = pretrained_layer.weight\n",
    "                        W_ft = finetuned_layer.weight\n",
    "                        bias_pre = pretrained_layer.bias\n",
    "                        bias_ft = finetuned_layer.bias\n",
    "                    \n",
    "                    weight_mod = torch.norm(W_ft - W_pre).item()\n",
    "                    bias_mod = torch.norm(bias_ft - bias_pre).item() if bias_pre is not None else 0\n",
    "                    mod = weight_mod + bias_mod\n",
    "                    \n",
    "                layer_mods.append(mod)\n",
    "                \n",
    "            modifications[method_name] = layer_mods\n",
    "            print(f\"{method_name}: {layer_mods}\")\n",
    "    \n",
    "    return modifications\n",
    "\n",
    "def create_modification_heatmap(modifications, save_path=None, normalize_by='method'):\n",
    "    \"\"\"\n",
    "    Create layer-wise modification magnitude heatmap\n",
    "    \n",
    "    Args:\n",
    "        modifications: dict with method names as keys and lists of layer modifications as values\n",
    "        save_path: path to save the figure (optional)\n",
    "        normalize_by: 'method' (normalize each row) or 'layer' (normalize each column)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(modifications, index=[f\"Layer {i}\" for i in range(len(list(modifications.values())[0]))])\n",
    "    df = df.T  # Transpose so methods are rows\n",
    "    # Add average modification per layer\n",
    "    df['Average'] = df.mean(axis=1)\n",
    "    # Normalize based on choice\n",
    "    if normalize_by == 'method':\n",
    "        # Normalize each method (row) separately\n",
    "        df_norm = df.div(df.max(axis=1), axis=0).fillna(0)\n",
    "    elif normalize_by == 'layer':\n",
    "        # Normalize each layer (column) separately\n",
    "        df_norm = df.div(df.max(axis=0), axis=1).fillna(0)\n",
    "    else:\n",
    "        # No normalization\n",
    "        df_norm = df\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create custom colormap (white to red)\n",
    "    cmap = sns.color_palette(\"Reds\", as_cmap=True)\n",
    "    max_val = df.values.max()\n",
    "\n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(df_norm, \n",
    "                     cmap=cmap,\n",
    "                     cbar_kws={'label': 'Relative Modification Magnitude'},\n",
    "                     linewidths=0.5,\n",
    "                     annot=True,\n",
    "                     fmt='.2f',\n",
    "                     vmin=0,\n",
    "                     vmax=1.0 if normalize_by else max_val)\n",
    "    ax.vlines(len(df_norm.columns) - 1, *ax.get_ylim(), colors='black', linewidth=2, linestyles='dashed')\n",
    "    # Styling\n",
    "    plt.xlabel('Layer', fontsize=22, fontweight='bold')\n",
    "    plt.ylabel('Method', fontsize=22, fontweight='bold')\n",
    "    plt.title('Layer-wise Modification Magnitude', fontsize=24, fontweight='bold')\n",
    "    \n",
    "    # Rotate y-axis labels for better readability\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Heatmap saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "def analyze_modifications(modifications):\n",
    "    \"\"\"Print analysis of modification patterns\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(modifications)\n",
    "    \n",
    "    print(\"\\n=== Modification Analysis ===\")\n",
    "    print(\"\\nTotal modification per method:\")\n",
    "    total_mods = df.sum(axis=0).sort_values(ascending=False)\n",
    "    for method, total in total_mods.items():\n",
    "        print(f\"  {method}: {total:.4f}\")\n",
    "    \n",
    "    print(\"\\nAverage modification per layer:\")\n",
    "    avg_mods = df.mean(axis=1)\n",
    "    for layer_idx, avg in enumerate(avg_mods):\n",
    "        print(f\"  Layer {layer_idx}: {avg:.4f}\")\n",
    "    \n",
    "    print(\"\\nModification concentration (std/mean):\")\n",
    "    for method in df.columns:\n",
    "        concentration = df[method].std() / (df[method].mean() + 1e-8)\n",
    "        print(f\"  {method}: {concentration:.2f} {'(concentrated)' if concentration > 1 else '(distributed)'}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    cfg = configs.Configs()\n",
    "    cfg.device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    # Paths\n",
    "    BASE_PATH = '/data/dust/user/valentel/beegfs.migration/dust/logs/MyCaloTransfer_diffusionweights'\n",
    "    \n",
    "    # Load all models\n",
    "    print(\"Loading models...\")\n",
    "    pretrained_model, finetuned_models = load_finetuned_models(cfg, BASE_PATH, use_ema=True)\n",
    "    \n",
    "    # Calculate modifications\n",
    "    print(\"\\nCalculating modification magnitudes...\")\n",
    "    modifications = calculate_modification_magnitude(pretrained_model, finetuned_models)\n",
    "    \n",
    "    # Analyze modifications\n",
    "    analyze_modifications(modifications)\n",
    "    \n",
    "    # Create heatmap\n",
    "    print(\"\\nCreating modification heatmap...\")\n",
    "    df_norm = create_modification_heatmap(\n",
    "        modifications, \n",
    "        # save_path='layer_modification_heatmap.pdf',\n",
    "        normalize_by=None  # or 'layer' or None\n",
    "    )\n",
    "    \n",
    "    # Optional: Save raw modification values\n",
    "    df_raw = pd.DataFrame(modifications)\n",
    "    df_raw.to_csv('modification_magnitudes.csv')\n",
    "    print(\"\\nRaw modification values saved to modification_magnitudes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import os\n",
    "\n",
    "def inverse_lora_experiment(pretrained_model, finetuned_models, num_layers: int = 6,\n",
    "                           save_path: str = './results/for_paper/lora/',\n",
    "                           error_threshold: Optional[float] = None,\n",
    "                           custom_ranks: Optional[List[int]] = None):\n",
    "    \"\"\"\n",
    "    Calculate the best LoRA approximation of known weight deltas using SVD.\n",
    "    Tests representation capacity vs optimization difficulty.\n",
    "    \n",
    "    Args:\n",
    "        pretrained_model: The pretrained base model\n",
    "        finetuned_models: Dictionary of finetuned model variants\n",
    "        num_layers: Number of layers to analyze\n",
    "        save_path: Directory to save figures\n",
    "        error_threshold: Optional error threshold to display (e.g., 0.05 for 5%)\n",
    "        custom_ranks: Optional list of ranks to test (defaults to powers of 2)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing reconstruction errors and optimal factors\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Inverse LoRA Experiment ===\")\n",
    "    \n",
    "    if 'Full FT' not in finetuned_models:\n",
    "        print(\"Full FT model not found, skipping inverse LoRA experiment\")\n",
    "        return {}\n",
    "        \n",
    "    full_ft_model = finetuned_models['Full FT']\n",
    "    inverse_results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for layer_idx in range(num_layers):\n",
    "            pretrained_layer = pretrained_model.diffusion.inner_model.layers[layer_idx]\n",
    "            finetuned_layer = full_ft_model.diffusion.inner_model.layers[layer_idx]\n",
    "            \n",
    "            # Extract weights\n",
    "            if hasattr(pretrained_layer, '_layer'):\n",
    "                W_pre = pretrained_layer._layer.weight\n",
    "                W_ft = finetuned_layer._layer.weight\n",
    "            else:\n",
    "                W_pre = pretrained_layer.weight\n",
    "                W_ft = finetuned_layer.weight\n",
    "            \n",
    "            # Compute true weight update\n",
    "            delta_W = W_ft - W_pre\n",
    "            \n",
    "            # SVD to get optimal low-rank approximation\n",
    "            U, S, V = torch.svd(delta_W)\n",
    "            \n",
    "            layer_results = {\n",
    "                'reconstruction_errors': {},\n",
    "                'optimal_BA': {},\n",
    "                'weight_shape': delta_W.shape,\n",
    "                'singular_values': S.cpu().numpy()  # Store for analysis\n",
    "            }\n",
    "            \n",
    "            # Generate test ranks more systematically\n",
    "            max_rank = min(delta_W.shape)\n",
    "            if custom_ranks is not None:\n",
    "                test_ranks = [r for r in custom_ranks if r <= max_rank]\n",
    "            else:\n",
    "                # Use powers of 2 plus some intermediate values for smoother curves\n",
    "                test_ranks = []\n",
    "                r = 1\n",
    "                while r <= max_rank:\n",
    "                    test_ranks.append(r)\n",
    "                    if r < max_rank:\n",
    "                        # Add intermediate point for smoother visualization\n",
    "                        intermediate = min(int(r * 1.5), max_rank)\n",
    "                        if intermediate not in test_ranks and intermediate < r * 2:\n",
    "                            test_ranks.append(intermediate)\n",
    "                    r *= 2\n",
    "                if max_rank not in test_ranks:\n",
    "                    test_ranks.append(max_rank)\n",
    "                test_ranks = sorted(test_ranks)\n",
    "            \n",
    "            for rank in test_ranks:\n",
    "                # Optimal low-rank approximation using truncated SVD\n",
    "                reconstruction = U[:, :rank] @ torch.diag(S[:rank]) @ V[:, :rank].T\n",
    "                \n",
    "                # Calculate relative Frobenius norm error\n",
    "                error = torch.norm(reconstruction - delta_W) / torch.norm(delta_W)\n",
    "                layer_results['reconstruction_errors'][rank] = error.item()\n",
    "                \n",
    "                # Store the LoRA factors for potential future use\n",
    "                B_optimal = U[:, :rank] * torch.sqrt(S[:rank].unsqueeze(0))\n",
    "                A_optimal = torch.sqrt(S[:rank].unsqueeze(1)) * V[:, :rank].T\n",
    "                layer_results['optimal_BA'][rank] = (B_optimal, A_optimal)\n",
    "                \n",
    "            inverse_results[f'Layer {layer_idx}'] = layer_results\n",
    "            \n",
    "            print(f\"\\nLayer {layer_idx} (shape: {delta_W.shape}) - Optimal LoRA reconstruction errors:\")\n",
    "            # Print key ranks only to avoid clutter\n",
    "            key_ranks = [r for r in [1, 4, 16, 64, 256, max_rank] if r in layer_results['reconstruction_errors']]\n",
    "            for rank in key_ranks:\n",
    "                error = layer_results['reconstruction_errors'][rank]\n",
    "                # Handle very small errors properly\n",
    "                if error < 0.0001:\n",
    "                    print(f\"  Rank {rank}: {error:.2e}\")\n",
    "                else:\n",
    "                    print(f\"  Rank {rank}: {error:.2%}\")\n",
    "    \n",
    "    # Create publication-quality visualization\n",
    "    create_reconstruction_plot(inverse_results, num_layers, save_path, error_threshold)\n",
    "    \n",
    "    # Optional: Create singular value decay plot\n",
    "    create_singular_value_plot(inverse_results, num_layers, save_path)\n",
    "    \n",
    "    return inverse_results\n",
    "\n",
    "def create_reconstruction_plot(inverse_results: Dict, num_layers: int, \n",
    "                              save_path: str, error_threshold: Optional[float] = None):\n",
    "    \"\"\"Create publication-quality reconstruction error plot.\"\"\"\n",
    "    \n",
    "    # Set up the figure with publication-quality settings\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Use a professional color palette\n",
    "    colors = plt.cm.viridis(np.linspace(0.15, 0.85, num_layers))\n",
    "    \n",
    "    # Plot each layer's reconstruction error\n",
    "    for layer_idx in range(num_layers):\n",
    "        if f'Layer {layer_idx}' in inverse_results:\n",
    "            errors = inverse_results[f'Layer {layer_idx}']['reconstruction_errors']\n",
    "            ranks = sorted(errors.keys())\n",
    "            values = [errors[r] for r in ranks]\n",
    "            \n",
    "            plt.semilogy(ranks, values, \n",
    "                        marker='o', \n",
    "                        label=f'Layer {layer_idx}', \n",
    "                        linewidth=2.5,\n",
    "                        markersize=6,\n",
    "                        color=colors[layer_idx],\n",
    "                        markeredgewidth=0.5,\n",
    "                        markeredgecolor='white',\n",
    "                        alpha=0.9)\n",
    "    \n",
    "    # Use Unicode for y-axis label (more reliable than LaTeX)\n",
    "    plt.xlabel('LoRA Rank', fontsize=24, fontweight='normal')\n",
    "    plt.ylabel('Reconstruction Error [εᵣ]', fontsize=24, fontweight='normal')\n",
    "    \n",
    "    # Remove grid as requested\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Add subtle threshold line if specified\n",
    "    if error_threshold is not None:\n",
    "        ax = plt.gca()\n",
    "        plt.axhline(y=error_threshold, \n",
    "                   color='#C03221',\n",
    "                   linestyle='--', \n",
    "                   linewidth=1.5,\n",
    "                   alpha=0.7,\n",
    "                   zorder=0)\n",
    "        \n",
    "        # Get x limits after plotting\n",
    "        xlims = ax.get_xlim()\n",
    "        x_pos = 8  # 60% from the right edge\n",
    "\n",
    "        plt.text(x_pos, error_threshold * 1.5, \n",
    "        f'{error_threshold:.0%} error',\n",
    "        horizontalalignment='center',\n",
    "        fontsize=16,\n",
    "        color='#C03221',\n",
    "        alpha=0.9,)\n",
    "    \n",
    "    # Configure legend\n",
    "    plt.legend(loc='best', \n",
    "              frameon=False,\n",
    "              fontsize=20,\n",
    "              ncol=2 if num_layers > 6 else 1,\n",
    "              columnspacing=1.0,\n",
    "              handlelength=1.5)\n",
    "    \n",
    "    # Set reasonable x-axis limits\n",
    "    ax = plt.gca()\n",
    "    ax.set_xscale('log', base=2)  # Use log2 scale for ranks\n",
    "    \n",
    "    # Format tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "    \n",
    "    # Ensure tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    filepath = os.path.join(save_path, 'inverse_lora_reconstruction_error.pdf')\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved reconstruction error plot to {filepath}\")\n",
    "    plt.show()\n",
    "\n",
    "def create_singular_value_plot(inverse_results: Dict, num_layers: int, save_path: str):\n",
    "    \"\"\"\n",
    "    Create plot showing singular value decay for each layer.\n",
    "    \n",
    "    This plot reveals the intrinsic dimensionality of the weight updates\n",
    "    and explains why certain LoRA ranks work better than others.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = plt.cm.viridis(np.linspace(0.15, 0.85, num_layers))\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        if f'Layer {layer_idx}' in inverse_results:\n",
    "            singular_values = inverse_results[f'Layer {layer_idx}']['singular_values']\n",
    "            \n",
    "            # Normalize by first singular value for comparison across layers\n",
    "            normalized_sv = singular_values / singular_values[0]\n",
    "            \n",
    "            plt.semilogy(range(1, len(normalized_sv) + 1), \n",
    "                        normalized_sv,\n",
    "                        linewidth=2.5,\n",
    "                        label=f'Layer {layer_idx}',\n",
    "                        color=colors[layer_idx],\n",
    "                        alpha=0.9)\n",
    "    \n",
    "    plt.xlabel('Singular Value Index', fontsize=24)\n",
    "    plt.ylabel('Normalized Singular Value', fontsize=24)\n",
    "    \n",
    "    # No grid as requested\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Add reference lines for interpretation\n",
    "    plt.axhline(y=0.1, color='gray', linestyle=':', linewidth=1, alpha=0.7)\n",
    "    plt.axhline(y=0.01, color='gray', linestyle=':', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Add text annotations for reference lines\n",
    "    ax = plt.gca()\n",
    "    xlims = ax.get_xlim()\n",
    "    plt.text(xlims[1] * 0.98, 0.11, '10%', \n",
    "            horizontalalignment='right', fontsize=12, color='gray', alpha=0.7)\n",
    "    plt.text(xlims[1] * 0.98, 0.011, '1%', \n",
    "            horizontalalignment='right', fontsize=12, color='gray', alpha=0.7)\n",
    "    \n",
    "    plt.legend(loc='best', frameon=False, fontsize=14)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    filepath = os.path.join(save_path, 'singular_value_decay.pdf')\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved singular value decay plot to {filepath}\")\n",
    "    plt.show()\n",
    "\n",
    "def analyze_effective_rank(inverse_results: Dict, energy_threshold: float = 0.95):\n",
    "    \"\"\"\n",
    "    Analyze the effective rank needed to capture a given percentage of the update energy.\n",
    "    \n",
    "    Args:\n",
    "        inverse_results: Results from inverse_lora_experiment\n",
    "        energy_threshold: Percentage of energy to capture (0.95 = 95%)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with effective ranks for each layer\n",
    "    \"\"\"\n",
    "    effective_ranks = {}\n",
    "    \n",
    "    for layer_name, layer_data in inverse_results.items():\n",
    "        if 'singular_values' in layer_data:\n",
    "            sv = layer_data['singular_values']\n",
    "            \n",
    "            # Calculate cumulative energy (sum of squared singular values)\n",
    "            sv_squared = sv ** 2\n",
    "            cumulative_energy = np.cumsum(sv_squared) / np.sum(sv_squared)\n",
    "            \n",
    "            # Find rank that captures desired energy\n",
    "            effective_rank = np.argmax(cumulative_energy >= energy_threshold) + 1\n",
    "            \n",
    "            effective_ranks[layer_name] = {\n",
    "                'effective_rank': effective_rank,\n",
    "                'total_rank': len(sv),\n",
    "                'energy_at_effective_rank': cumulative_energy[effective_rank - 1],\n",
    "                'compression_ratio': len(sv) / effective_rank\n",
    "            }\n",
    "            \n",
    "            print(f\"{layer_name}: Rank {effective_rank}/{len(sv)} captures {energy_threshold:.0%} of energy \"\n",
    "                  f\"(compression ratio: {len(sv)/effective_rank:.1f}x)\")\n",
    "    \n",
    "    return effective_ranks\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main experiment\n",
    "    results = inverse_lora_experiment(\n",
    "        pretrained_model=pretrained_model,\n",
    "        finetuned_models=finetuned_models,\n",
    "        num_layers=6,\n",
    "        save_path='./results/for_paper/lora/',\n",
    "        error_threshold=0.05,  # Show 5% error line if justified\n",
    "        custom_ranks=None  # Use automatic rank selection\n",
    "    )\n",
    "    \n",
    "    # Analyze effective ranks\n",
    "    print(\"\\n=== Effective Rank Analysis ===\")\n",
    "    effective_ranks = analyze_effective_rank(results, energy_threshold=0.95)\n",
    "    \n",
    "    print(\"\\n=== Effective Rank Analysis (99% energy) ===\")\n",
    "    effective_ranks_99 = analyze_effective_rank(results, energy_threshold=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ShowerFlow only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_finals_sf import (\n",
    "    run_complete_analysis, \n",
    "    create_showerflow_config\n",
    ")\n",
    "\n",
    "# Define your experiment folders\n",
    "vanilla_folders = {\n",
    "    42: 'ShowerFlow_2025_08_08__13_30_54',\n",
    "    43: 'ShowerFlow_2025_05_06__18_00_45',\n",
    "    44: 'ShowerFlow_2025_08_08__16_37_03', \n",
    "    45: 'ShowerFlow_2025_08_09__12_41_00',\n",
    "    46: 'ShowerFlow_2025_08_09__17_09_12',\n",
    "}\n",
    "\n",
    "finetune_folders = {\n",
    "    42: 'ShowerFlow_2025_08_08__11_48_39',\n",
    "    43: 'ShowerFlow_2025_05_06__18_00_43',\n",
    "    44: 'ShowerFlow_2025_08_08__12_34_58',\n",
    "    45: 'ShowerFlow_2025_08_09__12_27_34',\n",
    "    46: 'ShowerFlow_2025_08_09__16_37_25',\n",
    "}\n",
    "\n",
    "# Create configuration\n",
    "config = create_showerflow_config(\n",
    "    vanilla_folders=vanilla_folders,\n",
    "    finetune_folders=finetune_folders\n",
    ")\n",
    "\n",
    "# Run the complete analysis - creates exactly 4 plots\n",
    "run_complete_analysis(\n",
    "    config=config,\n",
    "    metric_files=['KL_Features_all_epochs.json', 'WD_Features_all_epochs.json'],\n",
    "    output_dir=\"./results/for_paper/showerflow_analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.paths_trainings_cleaned as paths\n",
    "from utils.preprocessing_utils import read_hdf5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = paths.GEANT4_PATH\n",
    "\n",
    "# train_showers, train_incidents = read_hdf5_file(train_path)\n",
    "val_ds = read_hdf5_file(val_path)\n",
    "val_showers = np.array(val_ds['showers'])  # Convert to NumPy array\n",
    "val_incidents = np.array(val_ds['incident'])\n",
    "print(val_showers.shape)\n",
    "print(val_incidents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.preprocessing_utils import cylindrical_histogram, plt_scatter_2\n",
    "\n",
    "# Load data\n",
    "train_path = '/data/dust/user/valentel/maxwell.merged/MyCaloTransfer/CaloTransfer/data/calo-challenge/preprocessing/reduced_datasets/10-90GeV/47k_dset1-2-3_prep_10-90GeV.hdf5'\n",
    "train_ds = read_hdf5_file(train_path)\n",
    "train_showers = np.array(train_ds['showers'])  # Shape: (N_total, 3, M)\n",
    "train_incidents = np.array(train_ds['incident'])\n",
    "\n",
    "# Define coordinate ranges\n",
    "Xmin, Xmax = -18, 18\n",
    "Ymin, Ymax = 0, 45\n",
    "Zmin, Zmax = -18, 18\n",
    "\n",
    "# Transform normalized coordinates to physical coordinates\n",
    "train_showers[:, 0, :] = (train_showers[:, 0, :] + 1) * (Xmax - Xmin) / 2 + Xmin\n",
    "train_showers[:, 1, :] = (train_showers[:, 1, :] + 1) * (Ymax - Ymin) / 2 + Ymin\n",
    "train_showers[:, 2, :] = (train_showers[:, 2, :] + 1) * (Zmax - Zmin) / 2 + Zmin\n",
    "# train_showers[:, -1, :] = train_showers[:, -1, :] * 1000  # Scale energy to GeV\n",
    "plt_scatter_2(train_showers[-1])\n",
    "# Randomly sample 10,000 showers (without replacement)\n",
    "\n",
    "visible_energy = train_showers[:, 3, :][train_showers[:, 3, :] > 0]\n",
    "from utils.preprocessing_utils import plt_visible_e\n",
    "plt_visible_e(visible_energy, log_scale=True, title=' Before Rescaling')\n",
    "\n",
    "train_showers[:, 0, :] = (train_showers[:, 0, :] - Xmin) / (Xmax - Xmin) * 2 - 1\n",
    "train_showers[:, 1, :] = (train_showers[:, 1, :] - Ymin) / (Ymax - Ymin) * 2 - 1\n",
    "train_showers[:, 2, :] = (train_showers[:, 2, :] - Zmin) / (Zmax - Zmin) * 2 - 1\n",
    "\n",
    "n_samples = 10_000\n",
    "if len(train_showers) < n_samples:\n",
    "    raise ValueError(f\"Dataset has only {len(train_showers)} showers, but {n_samples} were requested.\")\n",
    "\n",
    "# Randomly select indices\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(train_showers), size=n_samples, replace=False)\n",
    "sampled_showers = train_showers[random_indices]  # Shape: (10000, 3, M)\n",
    "train_incidents = train_incidents[random_indices]  # Shape: (10000,)\n",
    "# Process each shower into cylindrical histograms (45 × 50 × 18) and flatten\n",
    "visible_energy = sampled_showers[:, 3, :][sampled_showers[:, 3, :] > 0]\n",
    "plt_visible_e(visible_energy, log_scale=True, title=' random sampled showers')\n",
    "\n",
    "train_showers_processed = np.zeros((n_samples, 45 * 50 * 18))  # Pre-allocate array\n",
    "for i in range(n_samples):\n",
    "    cyl_hist = cylindrical_histogram(train_showers[i])  # Expected output: (45, 50, 18)\n",
    "    hist_reshaped = cyl_hist.reshape(45 * 50 * 18)\n",
    "    train_showers_processed[i] = hist_reshaped  # Flatten to (40500,)\n",
    "\n",
    "print(\"Final shape:\", train_showers_processed.shape)  # Output: (10000, 40500)\n",
    "visible_energy = train_showers_processed[:, :][train_showers_processed[:, :] > 0]\n",
    "plt_visible_e(visible_energy, log_scale=True, title='After Rescaling')\n",
    "del train_showers, train_ds  # Free up memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have these variables already defined:\n",
    "# val_showers, train_showers_processed, val_incidents, train_incidents\n",
    "\n",
    "# Convert to NumPy arrays and normalize showers\n",
    "val_showers_norm = np.array(val_showers, dtype=np.float32) / 0.033\n",
    "train_showers_norm = np.array(train_showers_processed, dtype=np.float32) / 0.033\n",
    "\n",
    "# Create the splits (no shuffling)\n",
    "split_showers = [val_showers_norm, train_showers_norm]\n",
    "split_incidents = [np.array(val_incidents, dtype=np.float32), \n",
    "                   np.array(train_incidents, dtype=np.float32)]\n",
    "\n",
    "# Create showers_baseline_np as a numpy array of splits\n",
    "showers_baseline_np = np.array(split_showers)  # Shape: (2, N_samples, 40500)\n",
    "incidents_baseline_np = np.array(split_incidents)\n",
    "\n",
    "# Verify results\n",
    "print(\"Final outputs:\")\n",
    "print(f\"showers_baseline_np shape: {showers_baseline_np.shape}\")  # Should be (2, N, 40500)\n",
    "print(f\"incidents_baseline_np shape: {incidents_baseline_np.shape}\")\n",
    "\n",
    "print(\"\\nSplit details:\")\n",
    "print(f\"Split 1 (Validation): {showers_baseline_np[0].shape} showers, {incidents_baseline_np[0].shape} incidents\")\n",
    "print(f\"Split 2 (Training): {showers_baseline_np[1].shape} showers, {incidents_baseline_np[1].shape} incidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showers_baseline_np = np.array([showers_numpy_baseline])\n",
    "# incidents_baseline_np = np.array([incidents_numpy_baseline])\n",
    "# showers_baseline_np.shape, incidents_baseline_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline, wasserstein_dist_baseline = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.plot_evaluate as plot\n",
    "\n",
    "# Now pass this to the function\n",
    "kl_divergences_baseline, wasserstein_dist_baseline = plot.plot_visible_energy(\n",
    "    showers_baseline_np,  # Use the concatenated array\n",
    "    kl_divergences=kl_divergences_baseline,\n",
    "    wasserstein=wasserstein_dist_baseline,\n",
    "    log_scale=True\n",
    ")\n",
    "kl_divergences_baseline, wasserstein_dist_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline, wasserstein_dist_baseline = plot.plot_calibration_histograms(showers_baseline_np, incidents_baseline_np,\n",
    "                                            kl_divergences=kl_divergences_baseline, wasserstein=wasserstein_dist_baseline)\n",
    "kl_divergences_baseline, wasserstein_dist_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline, wasserstein_dist_baseline = plot.plot_energy_sum(showers_baseline_np, \n",
    "                                                                        kl_divergences=kl_divergences_baseline, \n",
    "                                                                        wasserstein=wasserstein_dist_baseline)\n",
    "kl_divergences_baseline, wasserstein_dist_baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline, wasserstein_dist_baseline = plot.plot_occupancy(showers_baseline_np,\n",
    "                                                                          kl_divergences=kl_divergences_baseline, wasserstein=wasserstein_dist_baseline)\n",
    "kl_divergences_baseline, wasserstein_dist_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline, wasserstein_dist_baseline= plot.plot_energy_layer(showers_baseline_np,\n",
    "                                                             kl_divergences=kl_divergences_baseline, wasserstein=wasserstein_dist_baseline)\n",
    "kl_divergences_baseline, wasserstein_dist_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline, wasserstein_dist_baseline = plot.plot_radial_energy(showers_baseline_np,\n",
    "                                                            kl_divergences=kl_divergences_baseline, \n",
    "                                                            wasserstein=wasserstein_dist_baseline)\n",
    "kl_divergences_baseline, wasserstein_dist_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_with_error_propagation(kl_divergences_baseline):\n",
    "    \"\"\"\n",
    "    Function to plot KL divergences with error propagation and an additional plot\n",
    "    for the average of averages and standard error propagation.\n",
    "    \n",
    "    Parameters:\n",
    "    kl_divergences_baseline : list-like\n",
    "        A nested list/array of KL divergence values for each dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame with the input data.\n",
    "    df = pd.DataFrame(kl_divergences_baseline, index=[f\"Shower {i}\" for i in range(2, len(kl_divergences_baseline) + 2)])\n",
    "\n",
    "    # Compute mean and standard deviation (error propagation)\n",
    "    means = df.mean()\n",
    "    errors = df.std()  # Standard deviation as error bars\n",
    "\n",
    "    # Plot means with standard deviation as error bars\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.errorbar(df.columns, means, yerr=errors, fmt='o', capsize=5, label=f\"means ± Std Dev\", color='b')\n",
    "\n",
    "    # Compute the average of averages\n",
    "    avg_of_averages = means.mean()\n",
    "\n",
    "    # Calculate the overall standard deviation and the number of total samples\n",
    "    total_std = means.std()\n",
    "    print(f\"Total Standard Deviation: {total_std:.4f}\")\n",
    "    N = len(df) * len(df.columns)\n",
    "\n",
    "    # Calculate the standard error for the average of averages\n",
    "    final_error = np.sqrt(total_std**2 / N)\n",
    "    \n",
    "    # Plot the average of averages with standard error\n",
    "    plt.axhline(y=avg_of_averages, color='r', linestyle='--', label=f\"Average of Averages: {avg_of_averages:.2f}\")\n",
    "    plt.fill_between(df.columns, avg_of_averages - final_error, avg_of_averages + final_error, color='r', alpha=0.2, label=\"Overall SEM\")\n",
    "\n",
    "    # Formatting\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Average Plot with Error Propagation and Average of Averages\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergences_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kl = plot.plot_dataframe(kl_divergences_baseline, 'KL Divergence')\n",
    "plot_with_error_propagation(kl_divergences_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wass = plot.plot_dataframe(wasserstein_dist_baseline,'Wasserstein distance')\n",
    "plot_with_error_propagation(wasserstein_dist_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wasserstein_dist_baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calo-transfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
